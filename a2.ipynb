{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObHPx1T8b3oS",
        "colab_type": "text"
      },
      "source": [
        "# CS 224n Assignment #2: word2vec\n",
        "## Understanding word2vec\n",
        "\n",
        "Let’s have a quick refresher on the word2vec algorithm. The key insight behind word2vec is that ‘a word is known by the company it keeps’. Concretely, suppose we have a ‘center’ word c and a contextual window surrounding c. We shall refer to words that lie in this contextual window as ‘outside words’. For example, in Figure 1 we see that the center word c is ‘banking’. Since the context window size is 2, the outside words are ‘turning’, ‘into’, ‘crises’, and ‘as’.\n",
        "The goal of the skip-gram word2vec algorithm is to accurately learn the probability distribution $P(O|C)$. Given a specific word $o$ and a specific word $c$, we want to calculate $P (O = o|C = c)$, which is the probability that word $o$ is an ‘outside’ word for $c$, i.e., the probability that $o$ falls within the contextual window of $c$.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1LUqUw8b5qXaN9iCLAjYZ_GHRJ4uDM6tH\" />\n",
        "\n",
        "In word2vec, the conditional probability distribution is given by taking vector dot-products and applying the softmax function:\n",
        "\n",
        "\\begin{equation*}\n",
        "P(O = o | C = c) = \\frac{\\exp(u_o^t v_c)}{\\sum_\\limits{w \\in Vocab} \\exp(u_w^tv_c)}\n",
        "\\tag{1}\n",
        "\\end{equation*}\n",
        "\n",
        "Here, $u_o$ is the ‘outside’ vector representing outside word $o$, and $v_c$ is the ‘center’ vector representing center word $c$. To contain these parameters, we have two matrices, $U$ and $V$ . The columns of $U$ are all the ‘outside’\n",
        "vectors $u_w$. The columns of $V$ are all of the ‘center’ vectors $v_w$. Both $U$ and $V$ contain a vector for every $w \\in$ Vocabulary$^{1}$<a id='note1'></a>.\n",
        "\n",
        "Recall from lectures that, for a single pair of words c and o, the loss is given by:\n",
        "\\begin{equation*} \n",
        "J_{naive-softmax}(v_c,o,U) = −\\log P(O = o|C = c).\n",
        "\\tag{2}\n",
        "\\end{equation*} \n",
        "Another way to view this loss is as the cross-entropy$^{2}$<a id='note2'></a> between the true distribution $y$ and the predicted distribution $\\hat{y}$. Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. Furthermore, the $k^{th}$ entry in these vectors indicates the conditional probability of the $k^{th}$ word being an ‘outside word’ for the given $c$. The true empirical distribution $y$ is a one-hot vector with a $1$ for the true outside word $o$, and $0$ everywhere else. The predicted distribution $\\hat{y}$ is the probability distribution $P (O|C = c)$ given by our model in equation $\\eqref{eq:eq1}$.\n",
        "\n",
        "Notes:\n",
        "\n",
        "* [1](#note1) Assume that every word in our vocabulary is matched to an integer number $k$. Bolded lowercase letters represent vectors. $u_k$ is both the $k^th$ column of $U$ and the ‘outside’ word vector for the word indexed by $k$. $v_k$ is both the $k^th$ column of V and the ‘center’ word vector for the word indexed by $k$. In order to simplify notation we shall interchangeably use $k$ to refer to the word and the index-of-the-word.\n",
        "* [2](#note2) The Cross Entropy Loss between the true (discrete) probability distribution p and another distribution $q$ is $−\\sum_{􏰀i} p_i \\log(q_i)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cJ0QeFghBde",
        "colab_type": "text"
      },
      "source": [
        "### (a)\n",
        "\n",
        "\\begin{equation}\n",
        "- \\sum\\limits_{w \\in {Vocab}} y_w \\log(\\hat{y}_w) = - \\sum\\limits_{w \\in {Vocab}} \\mathbb{1}_{w = o} \\log(\\hat{y}_w) = - \\log (\\hat{y}_o)\n",
        "\\tag{3}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuBFvEbcrWmq",
        "colab_type": "text"
      },
      "source": [
        "### (b)\n",
        "Remembering that:\n",
        "\n",
        "\\begin{equation}\n",
        "u_w = \\delta_w U^t \\text{ and } y = \\delta_o\n",
        "\\tag{3.b.1}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial v}\\left(\\exp(u^tv)\\right) = u \\cdot \\exp(u^tv)\n",
        "\\tag{3.b.2}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\frac{d}{d v}\\log(f(v)) = \\frac{1}{f(v)} \\frac{df}{dv}(v)\n",
        "\\tag{3.b.3}\n",
        "\\end{equation}\n",
        "\n",
        "If $s_j = \\frac{e^{x_j}}{\\sum_\\limits{k} e^{x_k}}$, then: \n",
        "\\begin{equation}\n",
        "\\frac{\\partial s_j}{\\partial x_i} =\n",
        "  \\begin{cases}\n",
        "    s_j(1-s_j) & \\text{if $i = j$} \\\\\n",
        "    -s_is_j & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\tag{3.b.4}\n",
        "\\end{equation}\n",
        "\n",
        "Then, from \\ref{eq:eq1} and \\ref{eq:eq2}, the chain-rule, then writing $x_w^c = u_w^tv_c$ : \n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial v_c} J_{naive-softmax} = - \\frac{\\partial}{\\partial v_c} \\log \\left( \\hat{y}_o \\right) = − \\frac{1}{s_o^c} \\frac{\\partial \\hat{y}_o} {\\partial v_c} = − \\frac{1}{\\hat{y}_o} \\sum\\limits_{w \\in Vocab} \\frac{\\partial x_w^c}{\\partial v_c} \\frac{\\partial \\hat{y}_o}{\\partial x_w^c}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial v_c} J_{naive-softmax} = − \\frac{1}{\\hat{y}_o} \\left( - \\sum\\limits_{\\substack w \\in Vocab \\\\ w \\neq o} u_w^t \\hat{y}_o \\hat{y}_w  + u_o^t \\hat{y}_o(1-\\hat{y}_o) \\right) = - u_o^t + \\sum\\limits_{w \\in Vocab} \\hat{y}_w u_w^t = -\\delta_o U^t + \\left(\\sum\\limits_{w \\in Vocab} \\hat{y}_w  \\delta_w\\right) U^t\n",
        "\\tag{3.b.5}\n",
        "\\end{equation}\n",
        "Finally:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial v_c} J_{naive-softmax} = U^t (\\hat{y}-y)\n",
        "\\tag{3.b}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyHOpQhBhDTp",
        "colab_type": "text"
      },
      "source": [
        "### c\n",
        "\n",
        "Adapting $\\ref{eq:eq3.b}$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial u_w} J_{naive-softmax} = − \\frac{1}{\\hat{y}_o} \\sum\\limits_{w' \\in Vocab} \\frac{\\partial x_{w'}^c}{\\partial u_w} \\frac{\\partial \\hat{y}_o}{\\partial x_{w'}^c}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial x_{w'}^c}{\\partial u_w} = \\frac{\\partial (u_{w'}^tv_c)}{\\partial u_w} = \n",
        "  \\begin{cases}\n",
        "    0 & \\text{if $w \\neq w'$}\\\\ \n",
        "    v_c & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "Then:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial u_w} J_{naive-softmax} = − \\frac{1}{\\hat{y}_o} v_c \\frac{\\partial \\hat{y}_o}{\\partial x_w^c} =\n",
        "  \\begin{cases}\n",
        "    − \\frac{1}{\\hat{y}_o} v_c \\hat{y}_o (1 - \\hat{y}_o) & = (\\hat{y}_o - 1) v_c & = (\\hat{y}_o - y_o) v_c & \\text{if $w = o$}\\\\ \n",
        "    \\frac{1}{\\hat{y}_o} v_c \\hat{y}_o \\hat{y}_w & = \\hat{y}_w v_c & = (\\hat{y}_w - y_w) v_c & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial u_w} J_{naive-softmax} = (\\hat{y} - y)v_c^t\n",
        "\\tag{3.c}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdCxa1iqoTV6",
        "colab_type": "text"
      },
      "source": [
        "### d\n",
        "\n",
        "The sigmoid function is given by:\n",
        "\\begin{equation}\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1}\n",
        "\\tag{4}\n",
        "\\end{equation}\n",
        "\n",
        "Then, the derivative of the sigmoid function with respect to the scalar $x$ is given by:\n",
        "\\begin{equation}\n",
        "\\frac{d\\sigma}{dx} = - \\frac{-e^{-x}}{{\\left(1 + e^{-x}\\right)}^2} = \\frac{1 +e^{-x} - 1}{{\\left(1 + e^{-x}\\right)}^2} = \\sigma(x)(1 - \\sigma(x))\n",
        "\\tag{4.d}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcVNK3QJx63Z",
        "colab_type": "text"
      },
      "source": [
        "### e\n",
        "Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that K negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1,w_2,\\dots,w_K$ and their outside vectors as $u_1,\\dots,u_K$. Note that $o \\notin \\left\\{w_1,\\dots,w_K\\right\\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:\n",
        "\\begin{equation}\n",
        "J_{neg-sample}(v_c,o,U) = − \\log\\left(\\sigma(u_o^t v_c)\\right) − \\sum\\limits_{k=1}^{K}\n",
        "\\log\\left(\\sigma(−u_k^t v_c)\\right)\n",
        "\\tag{5}\n",
        "\\end{equation}\n",
        "\n",
        "####(i)\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial v_c} J_{neg-sample}(v_c,o,U) = -\\frac{1}{\\sigma(x_o^c)} \\frac{\\partial x_o^c}{\\partial v_c} \\frac{d \\sigma}{d x}(x_o^c) - \\sum\\limits_{k=1}^{K} \\frac{1}{\\sigma(-x_k^c)} \\left(-\\frac{\\partial x_k^c}{\\partial v_c} \\frac{d \\sigma}{d x}(-x_k^c)\\right) = -(1 - \\sigma(x_o^c)) u_o^t + \\sum\\limits_{k=1}^{K} (1 - \\sigma(-x_k^c)) u_k^t\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial v_c} J_{neg-sample}(v_c,o,U) = -(1 - \\sigma(u_o^t v_c)) u_o^t + \\sum\\limits_{k=1}^{K} (1 - \\sigma(-u_k^t v_c)) u_k^t\n",
        "\\tag{5.1}\n",
        "\\end{equation}\n",
        "####(ii)\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial u_o} J_{neg-sample}(v_c,o,U) = -\\frac{1}{\\sigma(x_o^c)} \\frac{\\partial x_o^c}{\\partial u_o} \\frac{d \\sigma}{d x}(x_o^c) - \\sum\\limits_{k=1}^{K} \\frac{1}{\\sigma(-x_k^c)} \\left(-\\frac{\\partial x_k^c}{\\partial u_o} \\frac{d \\sigma}{d x}(-x_k^c)\\right) = -(1 - \\sigma(u_o^t v_c)) v_c\n",
        "\\tag{5.2}\n",
        "\\end{equation}\n",
        "####(iii)\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial u_k} J_{neg-sample}(v_c,o,U) = -\\frac{1}{\\sigma(x_o^c)} \\frac{\\partial x_o^c}{\\partial u_k} \\frac{d \\sigma}{d x}(x_o^c) - \\sum\\limits_{k'=1}^{K} \\frac{1}{\\sigma(-x_{k'}^c)} \\left(-\\frac{\\partial x_{k'}^c}{\\partial u_k} \\frac{d \\sigma}{d x}(-x_{k'}^c)\\right) = (1 - \\sigma(-u_k^t v_c)) v_c\n",
        "\\tag{5.2}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X6lQHJw08Gv",
        "colab_type": "text"
      },
      "source": [
        "### f\n",
        "\n",
        "Suppose the center word is $c = w_t$ and the context window is $\\left[w_{t−m}, \\dots, w_{t−1}, w_t, w_{t+1},\\dots, w_{t+m}\\right]$, where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:\n",
        "\\begin{equation}\n",
        "J_{skip-gram}(v_c,w_{t−m},\\dots,w_{t+m},U) = \\sum\\limits_{\\substack{−m \\leq j \\leq m \\\\ j\\neq0}}􏰁 J(v_c,w_{t+j},U)\n",
        "\\tag{6} \n",
        "\\end{equation}\n",
        "Here, $J(v_c,w_{t+j},U)$ represents an arbitrary loss term for the center word $c = w_t$ and outside word $w_{t+j}$. $J(v_c,w_{t+j},U)$ could be $J_{naive-softmax}(v_c,w_{t+j},U)$ or $J_{neg-sample}(v_c,w_{t+j},U)$, depending on your implementation.\n",
        "\n",
        "#### (i)\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial U} J_{skip-gram}(v_c,w_{t−m},\\dots,w_{t+m},U) = \\sum\\limits_{\\substack{−m \\leq j \\leq m \\\\ j\\neq0}}􏰁 \\frac{\\partial}{\\partial U} J(v_c,w_{t+j},U)\n",
        "\\tag{6.i}\n",
        "\\end{equation}\n",
        "#### (ii)\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial v_c} J_{skip-gram}(v_c,w_{t−m},\\dots,w_{t+m},U) = \\sum\\limits_{\\substack{−m \\leq j \\leq m \\\\ j\\neq0}}􏰁 \\frac{\\partial}{\\partial v_c} J(v_c,w_{t+j},U)\n",
        "\\tag{6.ii}\n",
        "\\end{equation}\n",
        "#### (iii)\n",
        "If $w \\neq c$:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial v_w} J_{skip-gram}(v_c,w_{t−m},\\dots,w_{t+m},U) = 0\n",
        "\\tag{6.iii}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8SMqBHIXWzx",
        "colab_type": "text"
      },
      "source": [
        "## Coding: Implementing word2vec\n",
        "\n",
        "### Word2Vec with skip-grams and Negative Sampling loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH-x5Uo-b2Du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils\n",
        "import numpy as np\n",
        "\n",
        "def normalizeRows(x):\n",
        "    \"\"\" Row normalization function\n",
        "\n",
        "    Implement a function that normalizes each row of a matrix to have\n",
        "    unit length.\n",
        "    \"\"\"\n",
        "    N = x.shape[0]\n",
        "    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N,1)) + 1e-30\n",
        "    return x\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute the softmax function for each row of the input x.\n",
        "    It is crucial that this function is optimized for speed because\n",
        "    it will be used frequently in later code. \n",
        "\n",
        "    Arguments:\n",
        "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
        "    Return:\n",
        "    x -- You are allowed to modify x in-place\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "\n",
        "    if len(x.shape) > 1:\n",
        "        # Matrix\n",
        "        tmp = np.max(x, axis=1)\n",
        "        x -= tmp.reshape((x.shape[0], 1))\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x, axis=1)\n",
        "        x /= tmp.reshape((x.shape[0], 1))\n",
        "    else:\n",
        "        # Vector\n",
        "        tmp = np.max(x)\n",
        "        x -= tmp\n",
        "        x = np.exp(x)\n",
        "        tmp = np.sum(x)\n",
        "        x /= tmp\n",
        "\n",
        "    assert x.shape == orig_shape\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdY6MnHtYVGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checks\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# First implement a gradient checker by filling in the following functions\n",
        "def gradcheck_naive(f, x, gradientText):\n",
        "    \"\"\" Gradient check for a function f.\n",
        "    Arguments:\n",
        "    f -- a function that takes a single argument and outputs the\n",
        "         loss and its gradients\n",
        "    x -- the point (numpy array) to check the gradient at\n",
        "    gradientText -- a string detailing some context about the gradient computation\n",
        "\n",
        "    Notes:\n",
        "    Note that gradient checking is a sanity test that only checks whether the\n",
        "    gradient and loss values produced by your implementation are consistent with\n",
        "    each other. Gradient check passing on its own doesn’t guarantee that you\n",
        "    have the correct gradients. It will pass, for example, if both the loss and\n",
        "    gradient values produced by your implementation are 0s (as is the case when\n",
        "    you have not implemented anything). Here is a detailed explanation of what\n",
        "    gradient check is doing if you would like some further clarification:\n",
        "    http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/. \n",
        "    \"\"\"\n",
        "    rndstate = random.getstate()\n",
        "    random.setstate(rndstate)\n",
        "    fx, grad = f(x) # Evaluate function value at original point\n",
        "    h = 1e-4        # Do not change this!\n",
        "\n",
        "    # Iterate over all indexes ix in x to check the gradient.\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        x[ix] += h # increment by h\n",
        "        random.setstate(rndstate)\n",
        "        fxh, _ = f(x) # evalute f(x + h)\n",
        "        x[ix] -= 2 * h # restore to previous value (very important!)\n",
        "        random.setstate(rndstate)\n",
        "        fxnh, _ = f(x)\n",
        "        x[ix] += h\n",
        "        numgrad = (fxh - fxnh) / 2 / h\n",
        "\n",
        "        # Compare gradients\n",
        "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
        "        if reldiff > 1e-5:\n",
        "            print(\"Gradient check failed for %s.\" % gradientText)\n",
        "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
        "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
        "                grad[ix], numgrad))\n",
        "            return\n",
        "\n",
        "        it.iternext() # Step to next dimension\n",
        "\n",
        "    print(\"Gradient check passed!. Read the docstring of the `gradcheck_naive`\"\n",
        "    \" method in utils.gradcheck.py to understand what the gradient check does.\")\n",
        "\n",
        "\n",
        "def grad_tests_softmax(skipgram, dummy_tokens, dummy_vectors, dataset):\n",
        "    print (\"======Skip-Gram with naiveSoftmaxLossAndGradient Test Cases======\")\n",
        "\n",
        "    # first test\n",
        "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
        "                skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
        "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
        "\n",
        "    assert np.allclose(output_loss, 11.16610900153398), \\\n",
        "           \"Your loss does not match expected loss.\"\n",
        "    expected_gradCenterVecs = [[ 0.,          0.,          0.        ],\n",
        "                               [ 0.,          0.,          0.        ],\n",
        "                               [-1.26947339, -1.36873189,  2.45158957],\n",
        "                               [ 0.,          0.,          0.        ],\n",
        "                               [ 0.,          0.,          0.        ]]\n",
        "    expected_gradOutsideVectors = [[-0.41045956,  0.18834851,  1.43272264],\n",
        "                                   [ 0.38202831, -0.17530219, -1.33348241],\n",
        "                                   [ 0.07009355, -0.03216399, -0.24466386],\n",
        "                                   [ 0.09472154, -0.04346509, -0.33062865],\n",
        "                                   [-0.13638384,  0.06258276,  0.47605228]]\n",
        "                     \n",
        "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
        "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
        "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
        "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
        "    print(\"The first test passed!\")\n",
        "\n",
        "    # second test\n",
        "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
        "                skipgram(\"b\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
        "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
        "    assert np.allclose(output_loss, 9.87714910003414), \\\n",
        "           \"Your loss does not match expected loss.\"\n",
        "    expected_gradCenterVecs = [[ 0.,          0.,          0.        ],\n",
        "                               [-0.14586705, -1.34158321, -0.29291951],\n",
        "                               [ 0.,          0.,          0.        ],\n",
        "                               [ 0.,          0.,          0.        ],\n",
        "                               [ 0.,          0.,          0.        ]]\n",
        "    expected_gradOutsideVectors = [[-0.30342672,  0.19808298,  0.19587419],\n",
        "                                   [-0.41359958,  0.27000601,  0.26699522],\n",
        "                                   [-0.08192272,  0.05348078,  0.05288442],\n",
        "                                   [ 0.6981188,  -0.4557458,  -0.45066387],\n",
        "                                   [ 0.10083022, -0.06582396, -0.06508997]]\n",
        "                     \n",
        "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
        "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
        "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
        "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
        "    print(\"The second test passed!\")\n",
        "\n",
        "    # third test\n",
        "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
        "                skipgram(\"a\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
        "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
        "\n",
        "    assert np.allclose(output_loss, 10.810758628593335), \\\n",
        "           \"Your loss does not match expected loss.\"\n",
        "    expected_gradCenterVecs = [[-1.1790274,  -1.35861865,  1.53590492],\n",
        "                               [ 0.,          0.,          0.        ],\n",
        "                               [ 0.,          0.,          0.        ],\n",
        "                               [ 0.,          0.,          0.        ],\n",
        "                               [ 0.,          0.,          0.        ]]\n",
        "    expected_gradOutsideVectors = [[-7.96035953e-01, -1.79609012e-02,  2.07761330e-01],\n",
        "                                   [ 1.40175316e+00,  3.16276545e-02, -3.65850437e-01],\n",
        "                                   [-1.99691259e-01, -4.50561933e-03,  5.21184016e-02],\n",
        "                                   [ 2.02560028e-02,  4.57034715e-04, -5.28671357e-03],\n",
        "                                   [-4.26281954e-01, -9.61816867e-03,  1.11257419e-01]]\n",
        "                                                     \n",
        "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
        "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
        "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
        "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
        "    print(\"The third test passed!\")\n",
        "\n",
        "    print(\"All 3 tests passed!\")\n",
        "\n",
        "\n",
        "def grad_tests_negsamp(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient):\n",
        "    print (\"======Skip-Gram with negSamplingLossAndGradient======\")  \n",
        "\n",
        "    # first test\n",
        "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
        "                skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:],\n",
        "                dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
        "\n",
        "    assert np.allclose(output_loss, 16.15119285363322), \\\n",
        "           \"Your loss does not match expected loss.\"\n",
        "    expected_gradCenterVecs = [[ 0.,          0.,          0.        ],\n",
        "                               [ 0.,          0.,          0.        ],\n",
        "                               [-4.54650789, -1.85942252,  0.76397441],\n",
        "                               [ 0.,          0.,          0.        ],\n",
        "                               [ 0.,          0.,          0.        ]]\n",
        "    expected_gradOutsideVectors = [[-0.69148188,  0.31730185,  2.41364029],\n",
        "                                   [-0.22716495,  0.10423969,  0.79292674],\n",
        "                                   [-0.45528438,  0.20891737,  1.58918512],\n",
        "                                   [-0.31602611,  0.14501561,  1.10309954],\n",
        "                                   [-0.80620296,  0.36994417,  2.81407799]]\n",
        "            \n",
        "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
        "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
        "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
        "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
        "    print(\"The first test passed!\")\n",
        "\n",
        "    # second test\n",
        "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
        "                skipgram(\"c\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:],\n",
        "                dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
        "    assert np.allclose(output_loss, 28.653567707668795), \\\n",
        "           \"Your loss does not match expected loss.\"\n",
        "    expected_gradCenterVecs = [  [ 0.,          0.,          0.        ],\n",
        "                                 [ 0.,          0.,          0.        ],\n",
        "                                 [-6.42994865, -2.16396482, -1.89240934],\n",
        "                                 [ 0.,          0.,          0.        ],\n",
        "                                 [ 0.,          0.,          0.        ]]\n",
        "    expected_gradOutsideVectors = [  [-0.80413277,  0.36899421,  2.80685192],\n",
        "                                     [-0.9277269,   0.42570813,  3.23826131],\n",
        "                                     [-0.7511534,   0.34468345,  2.62192569],\n",
        "                                     [-0.94807832,  0.43504684,  3.30929863],\n",
        "                                     [-1.12868414,  0.51792184,  3.93970919]]\n",
        "                     \n",
        "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
        "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
        "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
        "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
        "    print(\"The second test passed!\")\n",
        "\n",
        "    # third test\n",
        "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
        "                skipgram(\"a\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
        "                dummy_tokens, dummy_vectors[:5,:], \n",
        "                dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
        "    assert np.allclose(output_loss, 60.648705494891914), \\\n",
        "           \"Your loss does not match expected loss.\"\n",
        "    expected_gradCenterVecs = [  [-17.89425315,  -7.36940626,  -1.23364121],\n",
        "                                 [  0.,           0.,           0.        ],\n",
        "                                 [  0.,           0.,           0.        ],\n",
        "                                 [  0.,           0.,           0.        ],\n",
        "                                 [  0.,           0.,           0.        ]]\n",
        "    expected_gradOutsideVectors = [[-6.4780819,  -0.14616449,  1.69074639],\n",
        "                                   [-0.86337952, -0.01948037,  0.22533766],\n",
        "                                   [-9.59525734, -0.21649709,  2.5043133 ],\n",
        "                                   [-6.02261515, -0.13588783,  1.57187189],\n",
        "                                   [-9.69010072, -0.21863704,  2.52906694]]\n",
        "                                                     \n",
        "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
        "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
        "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
        "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
        "    print(\"The third test passed!\")\n",
        "\n",
        "    print(\"All 3 tests passed!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIe7uJo_YYvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4wv5MmwYcr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for the input here.\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array.\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE (~1 Line)\n",
        "    s = 1. / (1. + np.exp(-x))\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwVTrFCLYeQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naiveSoftmaxLossAndGradient(\n",
        "    centerWordVec,\n",
        "    outsideWordIdx,\n",
        "    outsideVectors,\n",
        "    dataset\n",
        "):\n",
        "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
        "\n",
        "    Implement the naive softmax loss and gradients between a center word's \n",
        "    embedding and an outside word's embedding. This will be the building block\n",
        "    for our word2vec models.\n",
        "\n",
        "    Arguments:\n",
        "    centerWordVec -- numpy ndarray, center word's embedding\n",
        "                    in shape (word vector length, )\n",
        "                    (v_c in the pdf handout)\n",
        "    outsideWordIdx -- integer, the index of the outside word\n",
        "                    (o of u_o in the pdf handout)\n",
        "    outsideVectors -- outside vectors is\n",
        "                    in shape (num words in vocab, word vector length) \n",
        "                    for all words in vocab (U in the pdf handout)\n",
        "    dataset -- needed for negative sampling, unused here.\n",
        "\n",
        "    Return:\n",
        "    loss -- naive softmax loss\n",
        "    gradCenterVec -- the gradient with respect to the center word vector\n",
        "                     in shape (word vector length, )\n",
        "                     (dJ / dv_c in the pdf handout)\n",
        "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
        "                    in shape (num words in vocab, word vector length) \n",
        "                    (dJ / dU)\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE (~6-8 Lines)\n",
        "\n",
        "    ### Please use the provided softmax function (imported earlier in this file)\n",
        "    ### This numerically stable implementation helps you avoid issues pertaining\n",
        "    ### to integer overflow. \n",
        "    v_c = np.expand_dims(centerWordVec, axis=-1) # rank = (K,1)\n",
        "    x_w = np.dot(outsideVectors, v_c) # u_k^t v_c ; rank = (V,K) * (K,1) = (V,1)\n",
        "\n",
        "    y_hat = softmax(x_w.T) # y_hat # rank = (V,1)\n",
        "    loss = -np.log(y_hat[0, outsideWordIdx])\n",
        "\n",
        "    dy = y_hat.T\n",
        "    dy[outsideWordIdx] -= 1.\n",
        "\n",
        "    gradCenterVec = np.squeeze(np.dot(outsideVectors.T, dy)) # rank = (K,V) * (V,1) = (K,1)\n",
        "    gradOutsideVecs = np.dot(dy, v_c.T) #rank = (V,1) * (1,K) = (V,K)\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return loss, gradCenterVec, gradOutsideVecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSYirptQYge1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
        "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
        "\n",
        "    negSampleWordIndices = [None] * K\n",
        "    for k in range(K):\n",
        "        newidx = dataset.sampleTokenIdx()\n",
        "        while newidx == outsideWordIdx:\n",
        "            newidx = dataset.sampleTokenIdx()\n",
        "        negSampleWordIndices[k] = newidx\n",
        "    return negSampleWordIndices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pnZbVoLYiL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def negSamplingLossAndGradient(\n",
        "    centerWordVec,\n",
        "    outsideWordIdx,\n",
        "    outsideVectors,\n",
        "    dataset,\n",
        "    K=10\n",
        "):\n",
        "    \"\"\" Negative sampling loss function for word2vec models\n",
        "\n",
        "    Implement the negative sampling loss and gradients for a centerWordVec\n",
        "    and a outsideWordIdx word vector as a building block for word2vec\n",
        "    models. K is the number of negative samples to take.\n",
        "\n",
        "    Note: The same word may be negatively sampled multiple times. For\n",
        "    example if an outside word is sampled twice, you shall have to\n",
        "    double count the gradient with respect to this word. Thrice if\n",
        "    it was sampled three times, and so forth.\n",
        "\n",
        "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
        "    \"\"\"\n",
        "\n",
        "    # Negative sampling of words is done for you. Do not modify this if you\n",
        "    # wish to match the autograder and receive points!\n",
        "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
        "    indices = [outsideWordIdx] + negSampleWordIndices\n",
        "\n",
        "    ### YOUR CODE HERE (~10 Lines)\n",
        "\n",
        "    ### Please use your implementation of sigmoid in here.\n",
        "    v_c = np.expand_dims(centerWordVec, axis=-1) # rank = (K,1)\n",
        "\n",
        "    u_o, u_W = outsideVectors[outsideWordIdx], outsideVectors[negSampleWordIndices]\n",
        "    sigmoid_o = sigmoid(np.dot(u_o, v_c))[0]\n",
        "    sigmoid_k = sigmoid(-np.dot(u_W, v_c)) # rank = (W,K)*(K,1) = (W,1)\n",
        "    loss = -np.log(sigmoid_o) - np.sum(np.log(sigmoid_k))\n",
        "\n",
        "    gradCenterVec = (\n",
        "        - (1 - sigmoid_o) * u_o # rank = (K,1)\n",
        "        + np.dot((1 - sigmoid_k).T, u_W) # rank = (1,W)*(W,K) = (K,1)\n",
        "    )\n",
        "\n",
        "    gradOutsideVecs = np.zeros_like(outsideVectors)\n",
        "    gradOutsideVecs_k = np.dot((1 - sigmoid_k), v_c.T) # rank = (W,1)*(1,K) = (W,K)\n",
        "    for idx, gradOutsideVec_k in zip(negSampleWordIndices, gradOutsideVecs_k):\n",
        "        gradOutsideVecs[idx] += gradOutsideVec_k\n",
        "    gradOutsideVecs[outsideWordIdx] += np.squeeze(-(1 - sigmoid_o) * v_c)\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return loss, gradCenterVec, gradOutsideVecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANjinOjGYm-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "\n",
        "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
        "             centerWordVectors, outsideVectors, dataset,\n",
        "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
        "    \"\"\" Skip-gram model in word2vec\n",
        "\n",
        "    Implement the skip-gram model in this function.\n",
        "\n",
        "    Arguments:\n",
        "    currentCenterWord -- a string of the current center word\n",
        "    windowSize -- integer, context window size\n",
        "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
        "    word2Ind -- a dictionary that maps words to their indices in\n",
        "              the word vector list\n",
        "    centerWordVectors -- center word vectors (as rows) is in shape \n",
        "                        (num words in vocab, word vector length) \n",
        "                        for all words in vocab (V in pdf handout)\n",
        "    outsideVectors -- outside vectors is in shape \n",
        "                        (num words in vocab, word vector length) \n",
        "                        for all words in vocab (U in the pdf handout)\n",
        "    word2vecLossAndGradient -- the loss and gradient function for\n",
        "                               a prediction vector given the outsideWordIdx\n",
        "                               word vectors, could be one of the two\n",
        "                               loss functions you implemented above.\n",
        "\n",
        "    Return:\n",
        "    loss -- the loss function value for the skip-gram model\n",
        "            (J in the pdf handout)\n",
        "    gradCenterVec -- the gradient with respect to the center word vector\n",
        "                     in shape (word vector length, )\n",
        "                     (dJ / dv_c in the pdf handout)\n",
        "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
        "                    in shape (num words in vocab, word vector length) \n",
        "                    (dJ / dU)\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0.0\n",
        "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
        "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
        "\n",
        "    ### YOUR CODE HERE (~8 Lines)\n",
        "    c = word2Ind[currentCenterWord]\n",
        "    w = [word2Ind[outsideWord] for outsideWord in outsideWords]\n",
        "    v_c = centerWordVectors[c,:]\n",
        "    gradCenterVec = np.zeros(centerWordVectors.shape[1:])\n",
        "\n",
        "    loss, gradCenterVec, gradOutsideVectors = functools.reduce(\n",
        "        lambda X,Y : tuple(xi + yi for xi,yi in zip(X,Y)),\n",
        "        (word2vecLossAndGradient(v_c, w_k, outsideVectors, dataset) for w_k in w),\n",
        "        (loss, gradCenterVecs[c,:], gradOutsideVectors)\n",
        "    )\n",
        "    gradCenterVecs[c] = gradCenterVec\n",
        "\n",
        "    ### END YOUR CODE\n",
        "    \n",
        "    return loss, gradCenterVecs, gradOutsideVectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7bCKdK9YpFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d87dc9ec-14b1-4d92-c7f2-86ec1f838e76"
      },
      "source": [
        "#############################################\n",
        "# Testing functions below. DO NOT MODIFY!   #\n",
        "#############################################\n",
        "\n",
        "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset, \n",
        "                         windowSize,\n",
        "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
        "    batchsize = 50\n",
        "    loss = 0.0\n",
        "    grad = np.zeros(wordVectors.shape)\n",
        "    N = wordVectors.shape[0]\n",
        "    centerWordVectors = wordVectors[:int(N/2),:]\n",
        "    outsideVectors = wordVectors[int(N/2):,:]\n",
        "    for i in range(batchsize):\n",
        "        windowSize1 = random.randint(1, windowSize)\n",
        "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
        "\n",
        "        c, gin, gout = word2vecModel(\n",
        "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
        "            outsideVectors, dataset, word2vecLossAndGradient\n",
        "        )\n",
        "        loss += c / batchsize\n",
        "        grad[:int(N/2), :] += gin / batchsize\n",
        "        grad[int(N/2):, :] += gout / batchsize\n",
        "\n",
        "    return loss, grad\n",
        "\n",
        "def test_word2vec():\n",
        "    \"\"\" Test the two word2vec implementations, before running on Stanford Sentiment Treebank \"\"\"\n",
        "    dataset = type('dummy', (), {})()\n",
        "    def dummySampleTokenIdx():\n",
        "        return random.randint(0, 4)\n",
        "\n",
        "    def getRandomContext(C):\n",
        "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
        "        return tokens[random.randint(0,4)], \\\n",
        "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
        "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
        "    dataset.getRandomContext = getRandomContext\n",
        "\n",
        "    random.seed(31415)\n",
        "    np.random.seed(9265)\n",
        "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
        "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
        "\n",
        "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
        "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
        "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
        "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
        "    grad_tests_softmax(skipgram, dummy_tokens, dummy_vectors, dataset)\n",
        "\n",
        "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
        "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
        "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
        "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
        "\n",
        "    grad_tests_negsamp(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient)\n",
        "\n",
        "test_word2vec()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
            "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
            "======Skip-Gram with naiveSoftmaxLossAndGradient Test Cases======\n",
            "The first test passed!\n",
            "The second test passed!\n",
            "The third test passed!\n",
            "All 3 tests passed!\n",
            "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
            "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
            "======Skip-Gram with negSamplingLossAndGradient======\n",
            "The first test passed!\n",
            "The second test passed!\n",
            "The third test passed!\n",
            "All 3 tests passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLFKQqJP8cCT",
        "colab_type": "text"
      },
      "source": [
        "### Stochastic Gradient Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOo_VuaGYuxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save parameters every a few SGD iterations as fail-safe\n",
        "SAVE_PARAMS_EVERY = 5000\n",
        "\n",
        "import pickle\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path as op\n",
        "\n",
        "def load_saved_params():\n",
        "    \"\"\"\n",
        "    A helper function that loads previously saved parameters and resets\n",
        "    iteration start.\n",
        "    \"\"\"\n",
        "    st = 0\n",
        "    for f in glob.glob(\"saved_params_*.npy\"):\n",
        "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
        "        if (iter > st):\n",
        "            st = iter\n",
        "\n",
        "    if st > 0:\n",
        "        params_file = \"saved_params_%d.npy\" % st\n",
        "        state_file = \"saved_state_%d.pickle\" % st\n",
        "        params = np.load(params_file)\n",
        "        with open(state_file, \"rb\") as f:\n",
        "            state = pickle.load(f)\n",
        "        return st, params, state\n",
        "    else:\n",
        "        return st, None, None\n",
        "\n",
        "\n",
        "def save_params(iter, params):\n",
        "    params_file = \"saved_params_%d.npy\" % iter\n",
        "    np.save(params_file, params)\n",
        "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
        "        pickle.dump(random.getstate(), f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmkuHG1V8892",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
        "        PRINT_EVERY=10):\n",
        "    \"\"\" Stochastic Gradient Descent\n",
        "\n",
        "    Implement the stochastic gradient descent method in this function.\n",
        "\n",
        "    Arguments:\n",
        "    f -- the function to optimize, it should take a single\n",
        "         argument and yield two outputs, a loss and the gradient\n",
        "         with respect to the arguments\n",
        "    x0 -- the initial point to start SGD from\n",
        "    step -- the step size for SGD\n",
        "    iterations -- total iterations to run SGD for\n",
        "    postprocessing -- postprocessing function for the parameters\n",
        "                      if necessary. In the case of word2vec we will need to\n",
        "                      normalize the word vectors to have unit length.\n",
        "    PRINT_EVERY -- specifies how many iterations to output loss\n",
        "\n",
        "    Return:\n",
        "    x -- the parameter value after SGD finishes\n",
        "    \"\"\"\n",
        "\n",
        "    # Anneal learning rate every several iterations\n",
        "    ANNEAL_EVERY = 20000\n",
        "\n",
        "    if useSaved:\n",
        "        start_iter, oldx, state = load_saved_params()\n",
        "        if start_iter > 0:\n",
        "            x0 = oldx\n",
        "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
        "\n",
        "        if state:\n",
        "            random.setstate(state)\n",
        "    else:\n",
        "        start_iter = 0\n",
        "\n",
        "    x = x0\n",
        "\n",
        "    if not postprocessing:\n",
        "        postprocessing = lambda x: x\n",
        "\n",
        "    exploss = None\n",
        "\n",
        "    for iteration in range(start_iter + 1, iterations + 1):\n",
        "        # You might want to print the progress every few iterations.\n",
        "\n",
        "        loss = None\n",
        "        ### YOUR CODE HERE (~2 lines)\n",
        "        loss, gradient = f(x)\n",
        "        x -= step * gradient\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        x = postprocessing(x)\n",
        "        if iteration % PRINT_EVERY == 0:\n",
        "            if not exploss:\n",
        "                exploss = loss\n",
        "            else:\n",
        "                exploss = .95 * exploss + .05 * loss\n",
        "            print(\"iter %d: %f\" % (iteration, exploss))\n",
        "\n",
        "        if iteration % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
        "            save_params(iteration, x)\n",
        "\n",
        "        if iteration % ANNEAL_EVERY == 0:\n",
        "            step *= 0.5\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "391zrkEx89tP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "7f4e2946-d481-48aa-b523-04a3b100dfb2"
      },
      "source": [
        "def sanity_check():\n",
        "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
        "\n",
        "    print(\"Running sanity checks...\")\n",
        "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 1 result:\", t1)\n",
        "    assert abs(t1) <= 1e-6\n",
        "\n",
        "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 2 result:\", t2)\n",
        "    assert abs(t2) <= 1e-6\n",
        "\n",
        "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
        "    print(\"test 3 result:\", t3)\n",
        "    assert abs(t3) <= 1e-6\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(\"ALL TESTS PASSED\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "sanity_check()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running sanity checks...\n",
            "iter 100: 0.004578\n",
            "iter 200: 0.004353\n",
            "iter 300: 0.004136\n",
            "iter 400: 0.003929\n",
            "iter 500: 0.003733\n",
            "iter 600: 0.003546\n",
            "iter 700: 0.003369\n",
            "iter 800: 0.003200\n",
            "iter 900: 0.003040\n",
            "iter 1000: 0.002888\n",
            "test 1 result: 8.414836786079764e-10\n",
            "iter 100: 0.000000\n",
            "iter 200: 0.000000\n",
            "iter 300: 0.000000\n",
            "iter 400: 0.000000\n",
            "iter 500: 0.000000\n",
            "iter 600: 0.000000\n",
            "iter 700: 0.000000\n",
            "iter 800: 0.000000\n",
            "iter 900: 0.000000\n",
            "iter 1000: 0.000000\n",
            "test 2 result: 0.0\n",
            "iter 100: 0.041205\n",
            "iter 200: 0.039181\n",
            "iter 300: 0.037222\n",
            "iter 400: 0.035361\n",
            "iter 500: 0.033593\n",
            "iter 600: 0.031913\n",
            "iter 700: 0.030318\n",
            "iter 800: 0.028802\n",
            "iter 900: 0.027362\n",
            "iter 1000: 0.025994\n",
            "test 3 result: -2.524451035823933e-09\n",
            "----------------------------------------\n",
            "ALL TESTS PASSED\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E3iajxz8vGu",
        "colab_type": "text"
      },
      "source": [
        "### Application to the Stanford Sentiment Treebank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkkQbd_p8vqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "2b473a13-ee49-4740-e5d3-256d2a34ba6e"
      },
      "source": [
        "! wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
        "! unzip stanfordSentimentTreebank.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-04 10:07:49--  http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip [following]\n",
            "--2020-03-04 10:07:49--  https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6372817 (6.1M) [application/zip]\n",
            "Saving to: ‘stanfordSentimentTreebank.zip’\n",
            "\n",
            "stanfordSentimentTr 100%[===================>]   6.08M  11.3MB/s    in 0.5s    \n",
            "\n",
            "2020-03-04 10:07:49 (11.3 MB/s) - ‘stanfordSentimentTreebank.zip’ saved [6372817/6372817]\n",
            "\n",
            "Archive:  stanfordSentimentTreebank.zip\n",
            "   creating: stanfordSentimentTreebank/\n",
            "  inflating: stanfordSentimentTreebank/datasetSentences.txt  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/stanfordSentimentTreebank/\n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSentences.txt  \n",
            "  inflating: stanfordSentimentTreebank/datasetSplit.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSplit.txt  \n",
            "  inflating: stanfordSentimentTreebank/dictionary.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._dictionary.txt  \n",
            "  inflating: stanfordSentimentTreebank/original_rt_snippets.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._original_rt_snippets.txt  \n",
            "  inflating: stanfordSentimentTreebank/README.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._README.txt  \n",
            "  inflating: stanfordSentimentTreebank/sentiment_labels.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._sentiment_labels.txt  \n",
            "  inflating: stanfordSentimentTreebank/SOStr.txt  \n",
            "  inflating: stanfordSentimentTreebank/STree.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XkIKxDg96t5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "class StanfordSentiment:\n",
        "    def __init__(self, path=None, tablesize = 1000000):\n",
        "        if not path:\n",
        "            path = \"stanfordSentimentTreebank\"\n",
        "\n",
        "        self.path = path\n",
        "        self.tablesize = tablesize\n",
        "\n",
        "    def tokens(self):\n",
        "        if hasattr(self, \"_tokens\") and self._tokens:\n",
        "            return self._tokens\n",
        "\n",
        "        tokens = dict()\n",
        "        tokenfreq = dict()\n",
        "        wordcount = 0\n",
        "        revtokens = []\n",
        "        idx = 0\n",
        "\n",
        "        for sentence in self.sentences():\n",
        "            for w in sentence:\n",
        "                wordcount += 1\n",
        "                if not w in tokens:\n",
        "                    tokens[w] = idx\n",
        "                    revtokens += [w]\n",
        "                    tokenfreq[w] = 1\n",
        "                    idx += 1\n",
        "                else:\n",
        "                    tokenfreq[w] += 1\n",
        "\n",
        "        tokens[\"UNK\"] = idx\n",
        "        revtokens += [\"UNK\"]\n",
        "        tokenfreq[\"UNK\"] = 1\n",
        "        wordcount += 1\n",
        "\n",
        "        self._tokens = tokens\n",
        "        self._tokenfreq = tokenfreq\n",
        "        self._wordcount = wordcount\n",
        "        self._revtokens = revtokens\n",
        "        return self._tokens\n",
        "\n",
        "    def sentences(self):\n",
        "        if hasattr(self, \"_sentences\") and self._sentences:\n",
        "            return self._sentences\n",
        "\n",
        "        sentences = []\n",
        "        with open(self.path + \"/datasetSentences.txt\", \"r\") as f:\n",
        "            first = True\n",
        "            for line in f:\n",
        "                if first:\n",
        "                    first = False\n",
        "                    continue\n",
        "\n",
        "                splitted = line.strip().split()[1:]\n",
        "                # Deal with some peculiar encoding issues with this file\n",
        "                sentences += [[w.lower() for w in splitted]]\n",
        "\n",
        "        self._sentences = sentences\n",
        "        self._sentlengths = np.array([len(s) for s in sentences])\n",
        "        self._cumsentlen = np.cumsum(self._sentlengths)\n",
        "\n",
        "        return self._sentences\n",
        "\n",
        "    def numSentences(self):\n",
        "        if hasattr(self, \"_numSentences\") and self._numSentences:\n",
        "            return self._numSentences\n",
        "        else:\n",
        "            self._numSentences = len(self.sentences())\n",
        "            return self._numSentences\n",
        "\n",
        "    def allSentences(self):\n",
        "        if hasattr(self, \"_allsentences\") and self._allsentences:\n",
        "            return self._allsentences\n",
        "\n",
        "        sentences = self.sentences()\n",
        "        rejectProb = self.rejectProb()\n",
        "        tokens = self.tokens()\n",
        "        allsentences = [[w for w in s\n",
        "            if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]]\n",
        "            for s in sentences * 30]\n",
        "\n",
        "        allsentences = [s for s in allsentences if len(s) > 1]\n",
        "\n",
        "        self._allsentences = allsentences\n",
        "\n",
        "        return self._allsentences\n",
        "\n",
        "    def getRandomContext(self, C=5):\n",
        "        allsent = self.allSentences()\n",
        "        sentID = random.randint(0, len(allsent) - 1)\n",
        "        sent = allsent[sentID]\n",
        "        wordID = random.randint(0, len(sent) - 1)\n",
        "\n",
        "        context = sent[max(0, wordID - C):wordID]\n",
        "        if wordID+1 < len(sent):\n",
        "            context += sent[wordID+1:min(len(sent), wordID + C + 1)]\n",
        "\n",
        "        centerword = sent[wordID]\n",
        "        context = [w for w in context if w != centerword]\n",
        "\n",
        "        if len(context) > 0:\n",
        "            return centerword, context\n",
        "        else:\n",
        "            return self.getRandomContext(C)\n",
        "\n",
        "    def sent_labels(self):\n",
        "        if hasattr(self, \"_sent_labels\") and self._sent_labels:\n",
        "            return self._sent_labels\n",
        "\n",
        "        dictionary = dict()\n",
        "        phrases = 0\n",
        "        with open(self.path + \"/dictionary.txt\", \"r\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line: continue\n",
        "                splitted = line.split(\"|\")\n",
        "                dictionary[splitted[0].lower()] = int(splitted[1])\n",
        "                phrases += 1\n",
        "\n",
        "        labels = [0.0] * phrases\n",
        "        with open(self.path + \"/sentiment_labels.txt\", \"r\") as f:\n",
        "            first = True\n",
        "            for line in f:\n",
        "                if first:\n",
        "                    first = False\n",
        "                    continue\n",
        "\n",
        "                line = line.strip()\n",
        "                if not line: continue\n",
        "                splitted = line.split(\"|\")\n",
        "                labels[int(splitted[0])] = float(splitted[1])\n",
        "\n",
        "        sent_labels = [0.0] * self.numSentences()\n",
        "        sentences = self.sentences()\n",
        "        for i in range(self.numSentences()):\n",
        "            sentence = sentences[i]\n",
        "            full_sent = \" \".join(sentence).replace('-lrb-', '(').replace('-rrb-', ')')\n",
        "            sent_labels[i] = labels[dictionary[full_sent]]\n",
        "\n",
        "        self._sent_labels = sent_labels\n",
        "        return self._sent_labels\n",
        "\n",
        "    def dataset_split(self):\n",
        "        if hasattr(self, \"_split\") and self._split:\n",
        "            return self._split\n",
        "\n",
        "        split = [[] for i in range(3)]\n",
        "        with open(self.path + \"/datasetSplit.txt\", \"r\") as f:\n",
        "            first = True\n",
        "            for line in f:\n",
        "                if first:\n",
        "                    first = False\n",
        "                    continue\n",
        "\n",
        "                splitted = line.strip().split(\",\")\n",
        "                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n",
        "\n",
        "        self._split = split\n",
        "        return self._split\n",
        "\n",
        "    def getRandomTrainSentence(self):\n",
        "        split = self.dataset_split()\n",
        "        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n",
        "        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n",
        "\n",
        "    def categorify(self, label):\n",
        "        if label <= 0.2:\n",
        "            return 0\n",
        "        elif label <= 0.4:\n",
        "            return 1\n",
        "        elif label <= 0.6:\n",
        "            return 2\n",
        "        elif label <= 0.8:\n",
        "            return 3\n",
        "        else:\n",
        "            return 4\n",
        "\n",
        "    def getDevSentences(self):\n",
        "        return self.getSplitSentences(2)\n",
        "\n",
        "    def getTestSentences(self):\n",
        "        return self.getSplitSentences(1)\n",
        "\n",
        "    def getTrainSentences(self):\n",
        "        return self.getSplitSentences(0)\n",
        "\n",
        "    def getSplitSentences(self, split=0):\n",
        "        ds_split = self.dataset_split()\n",
        "        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n",
        "\n",
        "    def sampleTable(self):\n",
        "        if hasattr(self, '_sampleTable') and self._sampleTable is not None:\n",
        "            return self._sampleTable\n",
        "\n",
        "        nTokens = len(self.tokens())\n",
        "        samplingFreq = np.zeros((nTokens,))\n",
        "        self.allSentences()\n",
        "        i = 0\n",
        "        for w in range(nTokens):\n",
        "            w = self._revtokens[i]\n",
        "            if w in self._tokenfreq:\n",
        "                freq = 1.0 * self._tokenfreq[w]\n",
        "                # Reweigh\n",
        "                freq = freq ** 0.75\n",
        "            else:\n",
        "                freq = 0.0\n",
        "            samplingFreq[i] = freq\n",
        "            i += 1\n",
        "\n",
        "        samplingFreq /= np.sum(samplingFreq)\n",
        "        samplingFreq = np.cumsum(samplingFreq) * self.tablesize\n",
        "\n",
        "        self._sampleTable = [0] * self.tablesize\n",
        "\n",
        "        j = 0\n",
        "        for i in range(self.tablesize):\n",
        "            while i > samplingFreq[j]:\n",
        "                j += 1\n",
        "            self._sampleTable[i] = j\n",
        "\n",
        "        return self._sampleTable\n",
        "\n",
        "    def rejectProb(self):\n",
        "        if hasattr(self, '_rejectProb') and self._rejectProb is not None:\n",
        "            return self._rejectProb\n",
        "\n",
        "        threshold = 1e-5 * self._wordcount\n",
        "\n",
        "        nTokens = len(self.tokens())\n",
        "        rejectProb = np.zeros((nTokens,))\n",
        "        for i in range(nTokens):\n",
        "            w = self._revtokens[i]\n",
        "            freq = 1.0 * self._tokenfreq[w]\n",
        "            # Reweigh\n",
        "            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n",
        "\n",
        "        self._rejectProb = rejectProb\n",
        "        return self._rejectProb\n",
        "\n",
        "    def sampleTokenIdx(self):\n",
        "        return self.sampleTable()[random.randint(0, self.tablesize - 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3gQYgZ09vBn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fa43298-4942-4530-bab3-9a0542ac93a0"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Check Python Version\n",
        "import sys\n",
        "assert sys.version_info[0] == 3\n",
        "assert sys.version_info[1] >= 5\n",
        "\n",
        "# Reset the random seed to make sure that everyone gets the same results\n",
        "random.seed(314)\n",
        "dataset = StanfordSentiment()\n",
        "tokens = dataset.tokens()\n",
        "nWords = len(tokens)\n",
        "\n",
        "# We are going to train 10-dimensional vectors for this assignment\n",
        "dimVectors = 10\n",
        "\n",
        "# Context size\n",
        "C = 5\n",
        "\n",
        "# Number of iterations\n",
        "ITERATIONS = 40000\n",
        "\n",
        "# Reset the random seed to make sure that everyone gets the same results\n",
        "random.seed(31415)\n",
        "np.random.seed(9265)\n",
        "\n",
        "startTime=time.time()\n",
        "wordVectors = np.concatenate(\n",
        "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
        "       dimVectors, np.zeros((nWords, dimVectors))),\n",
        "    axis=0)\n",
        "wordVectors = sgd(\n",
        "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
        "        negSamplingLossAndGradient),\n",
        "    wordVectors, 0.3, ITERATIONS, None, True, PRINT_EVERY=400)\n",
        "# Note that normalization is not called here. This is not a bug,\n",
        "# normalizing during training loses the notion of length.\n",
        "\n",
        "print(\"sanity check: cost at convergence should be around or below 10\")\n",
        "print(\"training took %d seconds\" % (time.time() - startTime))\n",
        "\n",
        "# concatenate the input and output word vectors\n",
        "wordVectors = np.concatenate(\n",
        "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
        "    axis=0)\n",
        "\n",
        "visualizeWords = [\n",
        "    \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
        "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
        "    \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
        "    \"hail\", \"coffee\", \"tea\"]\n",
        "\n",
        "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
        "visualizeVecs = wordVectors[visualizeIdx, :]\n",
        "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
        "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
        "U,S,V = np.linalg.svd(covariance)\n",
        "coord = temp.dot(U[:,0:2])\n",
        "\n",
        "for i in range(len(visualizeWords)):\n",
        "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
        "        bbox=dict(facecolor='green', alpha=0.1))\n",
        "\n",
        "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
        "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
        "\n",
        "plt.savefig('word_vectors.png')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 400: 23.788721\n",
            "iter 800: 23.651472\n",
            "iter 1200: 23.437040\n",
            "iter 1600: 23.423329\n",
            "iter 2000: 23.361609\n",
            "iter 2400: 23.270850\n",
            "iter 2800: 23.182961\n",
            "iter 3200: 22.921591\n",
            "iter 3600: 22.660957\n",
            "iter 4000: 22.358877\n",
            "iter 4400: 22.186541\n",
            "iter 4800: 21.785855\n",
            "iter 5200: 21.473992\n",
            "iter 5600: 21.239454\n",
            "iter 6000: 20.995119\n",
            "iter 6400: 20.880119\n",
            "iter 6800: 20.540802\n",
            "iter 7200: 20.289593\n",
            "iter 7600: 20.044701\n",
            "iter 8000: 19.758854\n",
            "iter 8400: 19.445766\n",
            "iter 8800: 19.025912\n",
            "iter 9200: 18.843652\n",
            "iter 9600: 18.543918\n",
            "iter 10000: 18.183247\n",
            "iter 10400: 17.893945\n",
            "iter 10800: 17.640432\n",
            "iter 11200: 17.337233\n",
            "iter 11600: 17.034636\n",
            "iter 12000: 16.838378\n",
            "iter 12400: 16.682625\n",
            "iter 12800: 16.468133\n",
            "iter 13200: 16.246599\n",
            "iter 13600: 15.994704\n",
            "iter 14000: 15.742924\n",
            "iter 14400: 15.559705\n",
            "iter 14800: 15.301844\n",
            "iter 15200: 15.117236\n",
            "iter 15600: 14.884666\n",
            "iter 16000: 14.626344\n",
            "iter 16400: 14.432468\n",
            "iter 16800: 14.115190\n",
            "iter 17200: 13.960216\n",
            "iter 17600: 13.772148\n",
            "iter 18000: 13.604191\n",
            "iter 18400: 13.353116\n",
            "iter 18800: 13.162832\n",
            "iter 19200: 12.992807\n",
            "iter 19600: 12.951957\n",
            "iter 20000: 12.824193\n",
            "iter 20400: 12.668114\n",
            "iter 20800: 12.600277\n",
            "iter 21200: 12.497501\n",
            "iter 21600: 12.352722\n",
            "iter 22000: 12.262171\n",
            "iter 22400: 12.112166\n",
            "iter 22800: 12.028569\n",
            "iter 23200: 11.967956\n",
            "iter 23600: 11.858721\n",
            "iter 24000: 11.717870\n",
            "iter 24400: 11.672659\n",
            "iter 24800: 11.536587\n",
            "iter 25200: 11.455251\n",
            "iter 25600: 11.379393\n",
            "iter 26000: 11.361207\n",
            "iter 26400: 11.246546\n",
            "iter 26800: 11.170501\n",
            "iter 27200: 11.051471\n",
            "iter 27600: 10.950918\n",
            "iter 28000: 10.915814\n",
            "iter 28400: 10.843146\n",
            "iter 28800: 10.831022\n",
            "iter 29200: 10.706812\n",
            "iter 29600: 10.659240\n",
            "iter 30000: 10.537610\n",
            "iter 30400: 10.495808\n",
            "iter 30800: 10.471543\n",
            "iter 31200: 10.432114\n",
            "iter 31600: 10.467945\n",
            "iter 32000: 10.493934\n",
            "iter 32400: 10.372921\n",
            "iter 32800: 10.360953\n",
            "iter 33200: 10.313822\n",
            "iter 33600: 10.296646\n",
            "iter 34000: 10.228125\n",
            "iter 34400: 10.213338\n",
            "iter 34800: 10.177693\n",
            "iter 35200: 10.168569\n",
            "iter 35600: 10.193499\n",
            "iter 36000: 10.171302\n",
            "iter 36400: 10.085124\n",
            "iter 36800: 10.100318\n",
            "iter 37200: 10.092373\n",
            "iter 37600: 10.129698\n",
            "iter 38000: 10.133342\n",
            "iter 38400: 10.064348\n",
            "iter 38800: 9.989171\n",
            "iter 39200: 9.974958\n",
            "iter 39600: 9.912160\n",
            "iter 40000: 9.867437\n",
            "sanity check: cost at convergence should be around or below 10\n",
            "training took 7018 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMHTqVDD_-VA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "ca819fd0-2a87-4f22-b3cb-aa1d6758d0f9"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"word_vectors.png\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd1hW5f/A8ffDHi7ADagp4AwXQ3CA\nAqKo2NdtDkoNLa1cOXKkpYazXImklpqKiiVOBBy4ITJ3OQgVcAOibHjg9we/nkJwBg8Cn9d1dV3P\nOc997vM5fL/X8/Hc5z73R5Gbm5uLEEIIUcpolHQAQgghxOuQBCaEEKJUkgQmhBCiVJIEJoQQolSS\nBCaEEKJUkgQmhBCiVJIEJoQQolSSBCaEEKJUkgQmhBCiVJIEJoQQolSSBCaEEKJUkgQmhBCiVJIE\nJoQQolSSBCaEEKJUkgQmhBCiVJIEJoQQolSSBCaEEKJUkgQmhBCiVJIEJoQQolSSBCaEEKJUkgQm\nhBCiVJIEJoQQolTSKukAhChOt+JukZ6ZXtJhFBs9HT3qmNYp6TCEKBGSwESZlp6ZToWqFUo6jGKT\n/DC5pEMQosTIEKIQQohSSRKYEEKIUkkSmBDPsWHtBrZv3l6kffbx6MO5M+cK7N+6aSvTJkwr0nMJ\nUZbJMzAhnmPo8KElHYIQ4hnkDkyUOzv8d9DNuRtubd2Y9OkklEollrUs8fnSB1dHV7p36s6D+w8A\nWDxvMb7LfAG4eP4i3Tt1x9XBleHvDudR4iNu/HUD9/buqr7/uv6Xavsbn2/wcPKgk30nJn0yidzc\n3HwxuLV1o5N9J36P/L1AjPEP4/lg8Ad4OHng4eTBr6d/Lc4/iRClkiQwUa5cu3KNXT/vYmfITkJO\nhKCpocnPW38mNSWVVratCD0ZSpu2bdj046YCx44dOZZpX04j9FQojZo0YonPEurVr0fFShW5eP4i\nkDcM2H9QfwDe836PfWH7OBR+iLS0NEKCQlR9paWlEXIihHlL5jFh9IQC55o5aSYfjP6AfWH7+P6n\n75k4ZmIx/UWEKL1kCFGUK8ePHOfC2Qt4OHsAkJ6WTtVqVdHR0cGtixsAb7d4m2OHj+U77nHSY5KS\nknBo5wBA33f7MtJrJADvDn2XbT9to/HXjdm9Yzd7Du8B4OSxk6z6dhVpaWk8SnxEw8YN6dy1MwA9\n+/QEoE3bNjx58oSkR0n5znfsyDGuXrmq2k5+kkxKcgqGFQyL+k8iRKklCUyUK7m5ufR9ty9TZ03N\nt993uS8KhQIATU1NsrOzX7pPj54eLPFZQluntrzd8m2MTYxJT0/n8/Gfsy9sH6Zmpiyet5iM9AzV\nMX+f61nbOTk57D64Gz09vVe9RCHKDRlCFOVKO+d27Nm5h4cPHgKQmJBI7K3YFx5XqXIlKlepTPjJ\ncCDvGVabtm0A0NPTw9nFmanjpqqGD/9OVsYmxqQkp7A3cG++/nb9vAuAiFMRVKpUiUqVK+X73qmT\nEz+s/kG1/fcQpRDiH3IHJsoVq0ZWDP94OL169iI3JxdNLU3GTR5HjmYOUTejALgXf4/HaY+JuhlF\nwpME0nLSiLoZxcRZE5k2dRoZGRnUNq3N1FlTVcfYOtmya+8uzC3NVfu69u5KB/sOGFc15q3Gb5Hw\nJIGom1GkZaeRpkyjg2MHcrJzmPzFZKJuRnE/4T5JqUlE3Yxi+KfDWeKzhM1bNqPMVtK8VXMmTiv4\nHCw1MVX1WZaVEuWNIvffU6PKkKpVq1KvXr2SDkOUsHRlOhoG+Qca0rPT0TTUfKnj7928h66BLlWq\nVXluu/g78ShzlFQ3rf7asb6OnLQcdDV08z6n5qCnKUOO4r+5ceMGDx8+LOkwXkqZvQOrV68ekZGR\nJR2GKGFXo68WWAsx6mYUhiYvngzx3bffcTr9NMvWLKOK0bMT2ITRE8h+nM3qDauf2644pCSk0KBO\nAyBvXUSrt6zUen5R9tjY2JR0CC+tzCawsk6dq6yX16Gpj8Z+xEdjP3phu8UrF6shGiHE0ySBlVLq\nXGVdVjwvPpfPXWbP9j1MmjOppEMRotSRBCZECWrSvAlNmjcp6TCEKJUkgQnx/8a/P557t++RmZHJ\nwBED6TW4F+0s2jFwxECOhR5DV0+XJT8swaSaCV+M/QLDCob8ce4P4h/E88n0T3Dt7kpubi5Lv1rK\nycMnQQEjPh1B556dmfnJTDp27UjHrh0BmDZ6Gm493KhQqQIbfTeydMNSVi9azd24u8TdiuNu3F0G\njhjIwBEDAfj+m+/Zv2M/VUyqULN2TRpZN2Loh7JOoyjfJIGVQxPHTMR7jDdWjcr+A389Hb0CQ6Cp\niamgKNj2s5mfUalyJTLSM/Ae4E0bxzakpaZhaWXJe97vsWrxKrau2YrXSC+yM7K59+gey35Yxq3o\nW0wdMxUHRweOhBzhj7N/sGbbGpISk/Ae4E3DRg1x7+bOto3bsLO3I/lJMmcjzjLpi0lcOHMBZaaS\nlIQUMtMyifoziqU/LCU1JZXB3Qfj0cODa39eI3RXKGu2r0GZrWR43+HUt6hPSkIKetoy61CUX2pN\nYEFBQXz66acolUpGjBjBlClT8n2fkZHB0KFD+e233zAxMWHr1q2qqfDnz59n5MiRPH78GA0NDX79\n9VdZpeA5cnNzyc3NRUOj4Lvqi1YsKoGISsazJp8U9vxw8bzF7N+zH4CH9x+Sm56Ljo4OQ4YMQaFQ\n0L5De44dPkaDOg2oaFiRbt27YVnPEst6ljxKfESDOg3YcG0DA4cMzJsN+Ba0c2pH0r0kevfuzYr5\nK6hiUIUTISfo2asnDes3JD4uHgN9AxrUaYBxZWO6eXajsUVjAKrXqE5FvYrcuXGHHu/0oIll3lCj\nRw8PqhpVVc0+FKK8UttKHEqlktGjR7N//34uX77Mli1buHz5cr42a9euxcjIiOvXrzNu3DgmT54M\nQHZ2NoMHD8bX15dLly5x5MgRtLW11RV6qRFzM4b2rdrzifcndLLvxITRE+jq1JWOdh1ZNPefpPXv\nelTPWoW9vDl57CTHjhxjd+huQk+G0sy6GRkZGWhpaz1ziSkdXR3V55d5nbLPwD7s8N/Btp+2MWDI\ngELb6Orqqj5ramqizFa+7iUJUeapLYFFRERgYWFB/fr10dHRYcCAAQQGBuZrExgYiJeXFwB9+vTh\n4MGD5ObmEhwcjLW1Nc2bNwfAxMQETc2XexG1vImOisZrhBeHIw4zc+5M9oftJ/RUKKdPnObyxcsF\n2r/MKuzlwZPHT6hcpTL6Bvpcv3qdM7+eea1+7B3t2bVjF0qlkviH8YSfDKdF6xYA9BvUjzWr1gC8\n0vCtbRtbQoJCSE9PJyU5hdCg0NeKTYiyRm1DiHFxcZibm6u2zczMCA8Pf2YbLS0tKleuTHx8PFev\nXkWhUODu7s6DBw8YMGAAkybJtOPCmNUxo7VdawB2/7KbTT9uQpmt5N7de1z78xpNmuWf8faiVdjL\nC2dXZzau3YiTjRMNLBvQyrbVa/XTtUdXfov4DTdHNxQKBdO+nEb1Gnmrc1SrXg1LK0vcu7u/oJf8\nWrRuQeeunXF1cKVa9Wo0btqYipUqvlZ8QpQlpWISR3Z2NsePH+fXX3/FwMAAFxcXWrdujYuLS752\nfn5++Pn5AfDgQfkcCjMwMADg1o1brF62mr1H9lLFqApjR40lPaPgi8/PGyIrT3R1dfnp558K7L92\n55rqc/d3utP9ne4AfOv7baHtFAoFM+bMYMacGQX6SktNIzoqmnf6vKPa59jeEcf2jgBM+Dx/XbBD\n4YdUn0d9MooJn08gLTWNXl17Yd3C+lUvUYgyR21DiKampsTExKi2Y2NjMTU1fWab7OxskpKSMDEx\nwczMjA4dOlC1alUMDAzw8PDgzJmCQzze3t5ERkYSGRlJtWrViveC3nBPnjxB31CfSpUr8eD+Aw6H\nHC7pkMq1o4eP4mTrxPsj3y+w8vzLmPTJJNzauuHe3h0PTw/ebvF2MUQpROmitjswW1tbrl27RnR0\nNKampvj7+7N58+Z8bTw9PVm/fj0ODg4EBATQqVMn1dDhggULSE1NRUdHh7CwMMaNG6eu0Eulpm83\npZl1Mzq07kBts9rYtrEt6ZDKtQ4dOxBxKeK1j1+5bmURRiNE2aC2BKalpcWKFStwd3dHqVQybNgw\nmjZtysyZM7GxscHT05Phw4czZMgQLCwsMDY2xt/fHwAjIyPGjx+Pra0tCoUCDw8PunXrpq7QSw3z\nuub5hp2eHub6W8C+ANXnZw2RCSHEm67MllOxsbEp06vRF7bKenEpa6ucq/Nvp05l7X8nUTJK02+n\nVGQWQghRKpWKWYhCFKXClpcqC/R0ZGUaUb5IAhPlTh3TOmqtp/YqymvtNSFehyQwUS6ps57aqyiL\nd4ZCFBdJYKWUOobBbt+7TUZWBrraui9u/AaRuxghygdJYKWUun6g38S7lBeRuxghygeZhSiEGp08\ndpJfw38t6TCEKBPkDkyIYhR7O5aMrAzV9t59e9E30Me4pnGh7VMTU1+pfxkuFeWZ3IGJVxJzM4ZO\n9p1e2G7hnIUcPXwUyF9/zL6ZPQnxCQB4unq+dhxbN23l7p27r318YVJTUhnSZwiujq50su/Eym9W\nMmLQCAAO7D1Ag+oNyMzMJD09HQdrBwBu/HWDQf8bRJcOXfif+/+4fvU6APEP4/lg8AcM7T+UUd6j\nuH7zOklpSezau4uAnwMYPmw4V6OvYmhimO8/A2MDKlSt8NL/vYkzKYVQF7kDE0VOqVTy2fTPXthu\nV+iu1z7H9k3badS4ETVr1XztPp52OPQwNWvVZGPARgAeJz3mpx/yVqgPPxlOw8YNOXfmHNnZ2bRs\n3RKASZ9OwucbH+pb1OfMr2eYOn4q2/dsZ+akmXww+gNMapvwJOMJo4ePZsf+HfQZ0AcDQwOGDB9S\nZHELUV5JAhOvLDs7mzHDx3Dh3AWsGluxbPUynO2c8ezlydHDR/no0484HHoY1y6uz11b0bKWJdfu\nXCMlOYX3B75P0qMksrOymTRjEu7d3Im5GcPg3oOxc7AjMjySmrVqss5/HQcPHOTc7+cYM2IMevp6\n7Ardhb6+/n++rkZNGvHltC+ZO3Murl1csXe0p+5bdbl25RpnfzuL9xhvTp84jVKpxM7RjpTkFH4L\n/42RXiNVfWRmZAJw7Mgxrl65SkZuBhr6GqQkp5Ca8mrDg0KI55MEJl5Z1LUoFq9cjG0bW8Z/NJ71\na9YDYGRsxIFjB4C8u5mXpauny9pNa6lYqSIJ8Qn06NSDzh6dgbwK0yvXrWTh8oWM9BrJvsB99B7Q\nmx/9fmTGnBk0b9W8yK6rgWUDgo4GcSj4EAu+WkA7p3bYO9pzKOQQWtpatO/YnrGjxpKjzGH6nOnk\n5ORQqXIlQk6EFOgrJyeH3Qd3E3cvDkMTQ9X+jPQMrl28JndgQhQBeQYmXtm/y7P06t+LiFN5ZUI8\ne73eM63c3Fx8Zvvg6uBKf8/+3L1zlwf38wqSmtc1p5l1MwCsW1gTcyvmeV39J3fv3EXfQJ/eA3oz\n6pNRXDh3AXtHe9Z8t4bWtq0xqWpCYkIiUdejaNSkERUrVcS8rjm7f9mtuo5LFy4B4NTJiR9W/6Dq\n+8ofVwDQ0NTg7OmzxXYNQpQncgcmXtnfFZyf3jYwNHit/n7e9jPx8fHsP7ofbW1t7JvZk5GeN3NP\nV/efl6g1NTVJTyu+SQt/XvqTOTPmoNBQoK2lzdfffI1VYyse3n9Im7ZtAGjSrAn3791XXfOKNSuY\nOm4qSxcuJTsrm569e9L07aZ8tfArPp/wOZu3bCZXJ5eWti2ZNnsaV89e5VH8IxwaONCxS0caNW1E\nyO4QMjMz6dilo+rObNjAYdyOu01GegbDPxzO4PcHF9t1C1FaSQITrywuJo7I8Ehs7G3YuX0ntg62\nXDx/8bX7e5L0hKpVq6Ktrc2JoyeIvRX7wmMMKxiSnFy0Lyw7uzrj7OpcYH/0w2jV5wXLFuT7rk69\nOmz6ZVOBY4xNjPH90Zeom1H5hhCnzJvC2KFj2XZ4G6eOnOLg3oNs2LeB3Nxcxr03jrORZ2lQpwGL\nVy7GyNiItLQ0ujl3w8PTA2OTwqfeC1FeyRCieGUNLBuw/vv1ONk4kfQoCa/hXv+pv179e3Hu93O4\ntHEhYEsAFlYWLzym36B+TBk7Bbe2bqSlpf2n85eU02GnOR12mnfd3mVQ50HcuH6D2Jt5yXud7zpc\nHV3p4dKD23G3iY6KfkFvQpQ/cgcmXol5XXOO/na0wP7wi+H5tv9dDfrfFaD/3e7vatDGJsbsPri7\n0PP9u8L0qE9GqT5369mNbj1Ld1XuXHJ5/+P36T2kt2pfSkIKJ4+d5NiRY+wO3Y2+gT59PPqQkZHx\nnJ6EKJ/UegcWFBREw4YNsbCwwMfHp8D3GRkZ9O/fHwsLC+zt7blx40a+72/dukWFChVYtGiRmiIW\nomgZGBqQkpwCgIOTA4H+garp9ffv3CcxPpEnj59QuUpl9A30uX71Omd+PVOSIQvxxlJbAlMqlYwe\nPZr9+/dz+fJltmzZwuXLl/O1Wbt2LUZGRly/fp1x48YxefLkfN+PHz+erl27qitkIYpcFeMqNLdt\nTr+O/Qg/Gk6X/3XhvR7v0a9TPyZ5TyI1JRVnV2eU2UqcbJyY98U8Wtm2KumwhXgjqW0IMSIiAgsL\nC+rXrw/AgAEDCAwMpEmTJqo2gYGBzJo1C4A+ffowZswYcnNzUSgU7Ny5k7feegtDQ8PCuhei1Jj3\n3bx82++OeFf1OSUhBV1dXX76+Sd1hyVEqaO2BBYXF4e5ublq28zMjPDw8Ge20dLSonLlysTHx6On\np8f8+fMJCQmR4cM3QNydONKz1LcGn562Hqa1TIu2TzXUU4P/X5xXUXD/vQf3yMzOLLBfR0vn1fpP\nKPrVPWSBYFFalIpJHLNmzWLcuHFUqPD82lR+fn74+fkB8ODBA3WE9kJvSun6ovxRSs9Kx9BYfXfC\nKQkpRd6nOn+gn1VTrUj+hrlFX7NN6qmJ0kJtCczU1JSYmH9WUYiNjcXU1LTQNmZmZmRnZ5OUlISJ\niQnh4eEEBAQwadIkHj16hIaGBnp6eowZMybf8d7e3nh7ewNgY2NT/Bf1Et6U0vWv86P0rLuU1IRU\nyC2KqF5OamIqyQYvH7+ejl4xRiOEeFOoLYHZ2tpy7do1oqOjMTU1xd/fn82bN+dr4+npyfr163Fw\ncCAgIIBOnTqhUCg4duyYqs2sWbOoUKFCgeQlit7z7lLUmZSTDZOxesuq0O8Ku8NNz0znavTVIo9D\nhtaEeLOoLYFpaWmxYsUK3N3dUSqVDBs2jKZNmzJz5kxsbGzw9PRk+PDhDBkyBAsLC4yNjfH391dX\neKKELZ63GMMKhvne9XoZ6rzDfZOG1hITEhnrPZa0x2ksXLSQ+/fus2juIqrVqEbA3oAXdyBEGaDW\nZ2AeHh54eHjk2/fll1+qPuvp6bF9+/bn9vH3LEUhSoPiGoY9duAYdU3rMnPdTMxqmzHof4NYuHwh\ndg52/yFaIUqXUjGJQ7z5tm/ezurlq0EBjZs2ZtL0SYwfPZ7E+ESMqxrzzXffYGpuSszNmEL3l1Wv\nOgz7Mn/HxMREvv/2e9LT0nn/z/fp2r0rEacjmDB6Ap09OvP57M+Z98U8Th07RWZmJl4feDFkWN4i\nwauWrmL3z7vJzMykS/cuTJw2sdiuXYjiJmshloCnS9cH7gjEvpk9i+Yuwr29Oy5tXFSl6RMTEhk2\ncBiuDq5079SdyxfzXv52aeNC0qMkcnNzaVq3Kds35925fuL9CUcPFVzqqThd+eMKSxcuZduebYSe\nDOXL+V8y/bPp9B3Yl9BTofTq14sZk2YAPHN/UYq5GUMn+05qP/a/etm/YzPrZkycNhHPXp6EnAhh\n/NTxNG/ZnBVrVjBjzgy2bNhCxUoV2Re2j71H9rJ5/WZu3bhF2MEwoqOi2XtkL8Engjl/9jynT5wu\nkWsVoihIAisBf5euDz0ZyqHwQ3R07QjkrQl44NgBhgwfgu8yXyDv2VAz62aEngplyhdT+HTkpwDY\ntLHh19O/cuWPK9StV1dVk+u3iN+wsVfvDMwTYSfo/r/uqtXSjYyN+C3iN/7X738A9B7QO198he0X\nr/Z3fJ6wQ2EEbAnAra0b3Tt1JzEhkeioaMIOhRF2KIzO7Trj3t6dqKtRskiwKNVkCLEEFFa6HqCr\nZ94yWdYtrNm/ez8AEacj+H7j9wC0c2pHYkLeWnn2DvaEnwwnNiaWoSOG8tMPP3Hn9h2qVKny3Lpc\nRfFeWnRsNAYp/5zjYdJDEpMTiboZpdqn1FDy162/0NLSIjs7G6WGkqibUc/cn/AkgTRlWr4+AHS1\ndamiU+WVY8zOzmbM8DFcOHcBq8ZWLFu9DN9lvoTsDyE9PR0bexvmL52PQqHg/O/nGT96PJBXiLLU\ny4U5C+cUKA1z5OARxowfoxpOFKK0kzuwEvB36fpGTRqx4KsFfOPzDfBP8UZNTU2U2crn9mHfNi+B\nRZyMwKGdAyZVTdi7cy92js9/iP/3rL3/8p+BsQGGJoaq/9p2bkvY8TCyNLIwNDEkWzOb5o7NOX7q\nOIYmhoSdCKNV+1YYmhg+c79ORR10K+nm69fQxJCMrNdbhT3qWhReH3gRFhlGxYoVWb9mPe95v8e+\nsH0cCj9EWloaIUEhAIz/aDxzFs4h9GToa52rqLR1asueX/aQEJ8A5A0f29jbEBgQCOQV/vz7HzvP\n4+TixIa1G8jKygLy/hapKak4uzizdeNW1WLCd27f4eGDh8V0NUIUP7kDKwF379ylilEVeg/oTaXK\nldiyYcsz29o72PPztp8ZN3kcJ4+dxNjEmIqVKlKxUkUS4hPIysqi7lt1sXOww3e5L3MXzVXjleRp\nYNmA4aOG4z3YG4WmgkaNGzF5xmRmTZ3FhrUbqGJchdlfzwZ45v6iVtusNrZtbIG8emPrfNdhXtec\nVd+uIi0tjUeJj2jYuCH2DvYkJSWpKi73HtCbwyGHiyWmF2nYuCGfTPyEPh590NDUoJl1M+YsnMO4\nj8bhu8xXNYnjRd71epeYWzF0ad+F3NxcjKsas27zOpxcnLh25Rqerp5A3sr4y79fTtVqVYv70oQo\nFpLASkBhpeu9h3oX2nb81PFMGD0BVwdX9PT18tXZamnTkhxlDgB2jnZ8Petr1Y+2uvXo1YMevXrk\n27d6w+oC7WqZ1ip0/8hPRhZpPAqFosD25+M/Z1/YPkzNTFk8bzEZ6W9eja1+g/rRb1C/fPu27yn4\nakn/Qf3pP6i/avvfNdc0NDSY+sVUpn4xtcBxIz4awYiPRhRhxEKUHElgJaCw0vX/LvTYvFVz1Q+S\nkbER67asK7Sf5d8vV322tbclNim26IMtpeJi4ogMj8TG3oad23di62BLZHgkxibGpCSnsDdwL916\ndqNylcpUrlyZiFMR2DnY8cu2X0o6dCHES5IEJsqkBpYNWP/9eiaMnoBVIyu8hnuRlJiEi70L1WpU\no3mr5qq2S75bwvjR41EoFGVjEocQ5YQksHJu1dJV6OjoMPzD4Xwx5QsuX7zM9j3bOR52HP8N/rh2\ncWX54uXk5ubi4u7CtC+nAdDOoh19hvbhxKETVK1eldFTR7NszjLuxt1lwuwJOLk7cTvmNjM+nkFa\nahoAk+dOprltcyJPRuK32I8qxlW4/ud1Gls3Zs6KOQWG/V6XeV1zjv5W8F24yTMnM3nm5AL7rVta\n55vAMf2r6UUShxCieMksxHLOzsGO8FN5w5fnfz9PanIqWVlZRJyMoL5FfeZ+MZdte7YRfCKYs2fO\nErQnCIC01DRs29my/ch2DCoY8N3871jpv5JFaxfhuzDvHTYjEyO+8/+OzcGb8fH1YeGMharz/nnx\nTybMnkBAWABxt+I4G3FW/RcvhCjV5A6snLNuac2Fsxd48vgJOro6vN38bc6dOUf4qXDcurippugD\n9OrXi9MnTmNpbYm2jjaOHR0BsGhkgY6uDtra2lg0tuB27G0g712sBdMWcOXSFTQ1NLn5103VeZu1\naEaN2jUAsGpqxZ3YO7S0b6nmqxdClGaSwMo5bW1tzOuas23TNmzsbGjcrDEnj53kxl83MK9rzvmz\n5wsco6eth5amVl61YSA7IxttTW1V4UlllpKUhBR+XPkjFStUZO22teTk5ODW2o2UhBTSH6ejodBQ\ntc/JyiE1KbXQwpWpianoGZW9+l7qqgj9OqSemigtJIEVszflh+p5P0r2Dvb4Lvdl8crFNG7amNmf\nz8a6hTUtWrdgxqQZJMQnULlKZXYG7GTYyGGY1jJFoaGgQZ0GABhXNsawgqFq++/vtBRaWFlZYVnP\nkq0/bUWpVNKgTgPu3byHgb6Bqn3lCpWpZlJNtf1vyQbJZbIGV1m8JiHUTRJYMSsNP1R2jnYsW7QM\nGzsbDAwN0NXVxc7Rjho1a/D5rM/p262vahKHezf3l+7Xa4QX3kO8CdgSQEfXjs9d4koIIV6VIjc3\nV43F4dXHxsaGyMjIkg7jjXM1+qpaqyn/V8kPn12NGYpmbceXJRWZRXlQmn475Q5MlGqSUIQovySB\nlUNxd+JIz1LPXct/oaetR2XtyiUdhhDiDaXWBBYUFMSnn36KUqlkxIgRTJkyJd/3GRkZDB06lN9+\n+w0TExO2bt1KvXr1CAkJYcqUKWRmZqKjo8PChQvp1Klkig6WBelZ6RgaG5Z0GC+UkpAiCUwI8Uxq\ne5FZqVQyevRo9u/fz+XLl9myZQuXL1/O12bt2rUYGRlx/fp1xo0bx+TJeasmVK1ald27d3PhwgXW\nr1/PkCFSz0gIIco7tSWwiIgILCwsqF+/Pjo6OgwYMIDAwMB8bQIDA/Hy8gKgT58+HDx4kNzcXFq2\nbEnt2rUBaNq0KWlpaWRkvHkriQshhFAftQ0hxsXFYW5urto2MzMjPDz8mW20tLSoXLky8fHxVK36\nT72iHTt20KpVK1XxR/Fq9HT0SI1NhafmngZsCWDn9p1YNbZi5tyZRX7edb7r0DfQZ+DQgS99TGpi\nKnpV5KVaIUThStUkjkuXLjF58mSCg4ML/d7Pzw8/Pz8AHjx4oM7QSo06pnVUVZn/be/2vezYtYPa\nprWL5bzGFf//Zee6BV9WfuAiyYQAACAASURBVJZkw7L5ErMQomioLYGZmpoSExOj2o6NjcXU1LTQ\nNmZmZmRnZ5OUlISJiYmq/f/+9z82bNhAgwaF/wh6e3vj7Z1XGNLGxqaYrqTsmTx2Mrdu3GJI7yF4\n9vbkZvRNrly+QlZ2FhOmTsC9mztbN23lwJ4DpKamEh0VzaiPR5GZlckO/x3o6OiwMWAjRsZGbPpx\nE5t+2ERmViZv1X+LZX7L0DfQz3e+G3/dYNqEacTHx6Ovr8/C5QuxsLIooasXQpRWansGZmtry7Vr\n14iOjiYzMxN/f388PT3ztfH09GT9+vUABAQE0KlTJxQKBY8ePaJbt274+PjQtm1bdYVcbsz/dj41\natVg+97tpKam0rZDW/Ye2cv2Pdv5avpXpKbkrXl45fIV1vy0hn1H9jH/q/no6+sTfDyY1natCdiS\nV4Cza4+u7AvbR+jJUCysLNiyYUuB8036dBJfLfyKoKNBzJgzg6njC1YOFkKIF1HbHZiWlhYrVqzA\n3d0dpVLJsGHDaNq0KTNnzsTGxgZPT0+GDx/OkCFDsLCwwNjYGH9/fwBWrFjB9evX+fLLL/nyyy8B\nCA4Opnr16uoKv9w4eugoIftC8F2eVxIlIyODuNg4ABw7OFKhYgUqVKxAxUoVcevqBkDjpo25fDFv\nRumVP66w4KsFPE56TEpKCk4u+QtEpiSn8Fv4b4z0Gqnal5mRqY5LE0KUMWp9Bubh4YGHh0e+fX8n\nJAA9PT22b99e4Ljp06czfboUGVSH3Nxc/H7yw8Iy/5Demcgz6OjoqLY1NDRUE2kUGgqU2UoAxn04\njrWb19L07aZs3bSVU8dO5esnJyeHSpUrEXIipJivRAhR1klBS5GPk4sTP/j+wN9LZF48d/GVjk9+\nkkyNmjXIysril22/FPi+YqWKmNc1Z/cvu4G8hHnpwqX/HrgQotyRBCbyGTtpLFnZWbg6uNLRriML\n5ix4peM/m/4Z3Tt15x23dwrcxf1txZoV+G/wx9Ux7xzBewufVSqEEM8jq9GXQ6VlRfoXrUQvhCh6\npem3U+7AhBBClEqSwIQQQpRKksCEEEKUSqVqKSkhXoc6qza/DKnsLETRkAQmyrzC1n4sSckPk0s6\nBCHKBElg5ZCejt5L/4jevnebjCz1l67R1dalvnl9tZ9XCFF6SAIrh151+Kok7l6SH8pK9EKI55NJ\nHEK8pu9Xfk9aappq27KWZQlGI0T5IwlMiNegVCpZs2oNaWlpL24shCgWksBEubNq6SrWrloLwBdT\nvqBv974AHA87zpjhY9i5fScubVzoZN+JuTPnqo6zrGXJ7M9n4+royrKFy7h35x59u/WlT7c+qjY+\nX/rg6uhK907deXBfiqoKUZwkgYlyx87BjvBT4QCc//08qcmpZGVlEXEygvoW9Zn7xVy27dlG8Ilg\nzp45S9CeIABSU1JpadOS0JOhjJsyTlVDLWBvgOr7VratCD0ZSpu2bdj046YSu0YhygNJYOKVxdyM\noUPrDowdNZZ2LdsxZvgYjh4+Sk+3nrRt0ZbfI3/n98jf6eHSg87tOuPp6sn1a9cB2LppKyMGjWDQ\n/wbRtkVb5syYo/b4rVtac+HsBZ48foKOrg6t7Vpz7sw5wk+FU6lyJRzaOWBS1QQtLS169evF6ROn\nAdDU1KRbz27P7FdHRwe3Lnk10t5u8Taxt2LVcj1ClFeSwMRrufHXDUZ+PJKjvx3l+rXr7Ny+k53B\nO5k5dybLFy/HwsqCXw78QvDxYCZOm8j82fNVx166cIlVP67i4OmD7Pp5l6pgprpoa2tjXtecbZu2\nYWNng52jHSePneTGXzcwr2v+zON09XTR1NR85vda2looFAogL9llZ2cXeexCiH9IAhOvxbyuOY2b\nNkZDQwOrRla0c2qHQqGgUZNGxNyK4fHjx4wcOpJO9p2YPXU2V/64ojq2nVM7KlWuhJ6eHlYNrYiL\nUW8CA7B3sMd3uS/2be2xd7Rn47qNNLNuRovWLTh94jQJ8QkolUp2BuzEoZ1DoX1UqFCB5CfyUrIQ\nJUWtCSwoKIiGDRtiYWGBj49Pge8zMjLo378/FhYW2Nvbc+PGDdV3X3/9NRYWFjRs2JADBw6oMWpR\nmL+rMUNedWYdXR3VZ2W2koVzFuLY3pFD4Yf4ceuPZGT88zJ0vsrOmholcqdi52jH/bv3sbGzoVr1\naujq6mLnaEeNmjX4fNbn9O3WFzdHN6xbWOPezb3QPga9N4hBvQblm8QhhFAftb3IrFQqGT16NCEh\nIZiZmWFra4unpydNmjRRtVm7di1GRkZcv34df39/Jk+ezNatW7l8+TL+/v5cunSJ27dv4+rqytWr\nV587nCNK1pPHT6hZuyYA2zZtK+FoCmrv3J6bCTdV28d/P676/E7fd3in7zsFjrl251q+7WGjhjFs\n1LBCv+/+Tne6v9O9KEMWQjxFbXdgERERWFhYUL9+fXR0dBgwYACBgYH52gQGBuLl5QVAnz59OHjw\nILm5uQQGBjJgwAB0dXV56623sLCwICIiQl2hi9fw4acf8vWsr+ncrrM8CxJCFAu13YHFxcVhbv7P\nA3IzMzPCw8Of2UZLS4vKlSsTHx9PXFwcbdq0yXdsXJz6n5uIPOZ1zTkUfki1/a3vt4V+9++7mskz\nJwPQf1B/+g/qr9q/YfuG4g5XCFFGlam1EP38/PDz8wPgwQN5iVQIIcoytQ0hmpqaEhMTo9qOjY3F\n1NT0mW2ys7NJSkrCxMTkpY4F8Pb2JjIyksjISKpVq1ZMVyKEEOJNoLYEZmtry7Vr14iOjiYzMxN/\nf388PT3ztfH09GT9+vUABAQE0KlTJxQKBZ6envj7+5ORkUF0dDTXrl3Dzs5OXaELIYR4A6ltCFFL\nS4sVK1bg7u6OUqlk2LBhNG3alJkzZ2JjY4OnpyfDhw9nyJAhWFhYYGxsjL+/PwBNmzalX79+NGnS\nBC0tLVauXCkzEMVLe5X6Z+qgp6NX0iEIUSYocnNzc0s6iOJgY2NDZGRkSYdR6l2Nvlpi9cCs3rJS\n+3mFKO9K029nmZrEIYpeSd29yF2KEOJFJIGJ55KqyEKIN5WshSiEEKJUkgQmhBCiVJIhRCGEKMdu\nxd0iPTNdtZ2uTOdq9NUiP4+ejl6RP5KQBCaEEOVYemZ6vpnGGgYaxTLzuDgmg8kQohBCiFJJEpgQ\nQohSSRKYEEKIfGJuxtDJvtNLt9+wdgPbN28HYOyosezZuae4QstHnoEJIYT4T4YOH1oi55U7MCGE\nEAUolUo++/gzOtp1ZGDPgaSlpbHpx014OHng6ujKB4M/IC01DYDF8xbju8xX7TFKAhNCCFFAdFQ0\nXh94cTjiMJWqVGJf4D669ujKvrB9hJ4MxcLKgi0btpRojDKEKIQQogDzuuY0s24GgHULa2JuxXDl\njyss+GoBj5Mek5KSgpOLU4nGKAlMCCFEAbq6uqrPmpqapKelM+7DcazdvJambzdl66atnDp2qgQj\nlCFEIYQQLyn5STI1atYgKyuLX7b9UtLhyB2YEEKIl/PZ9M/o3qk7JiYmtLRpSXJyyRaKlYKWQghR\njj1dtLZLly4EBQUV+XmKo0it3IGJQj29wGdpVRwLiAoh3gxqSWAJCQn079+fGzduUK9ePbZt24aR\nkVGBduvXr2fOnDkATJ8+HS8vL1JTU+nbty9RUVFoamrSo0cPfHx81BF2ufb0Ap+lVUlUkxZCqIda\nJnH4+Pjg4uLCtWvXcHFxKTQBJSQkMHv2bMLDw4mIiGD27NkkJiYCMHHiRP78809+//13Tpw4wf79\n+9URthBCiDeYWhJYYGAgXl5eAHh5ebFz584CbQ4cOICbmxvGxsYYGRnh5uZGUFAQBgYGdOzYEQAd\nHR1atWpFbGysOsIWQgjxBlNLArt37x61atUCoGbNmty7d69Am7i4OMzNzVXbZmZmxMXF5Wvz6NEj\ndu/ejYuLS/EGLEqUp6tnSYcghCgFiuwZmKurK3fv3i2wf+7cufm2FQoFCoXilfvPzs5m4MCBfPLJ\nJ9SvX7/QNn5+fvj5+QHw4MGDVz6HKFpxd+JIz3r1iSDfrPuGqFtRRRJDakJqkfTzPDJRRJRmejp6\n+Z4V56TmFMuzYz0dvSLvs8gSWGho6DO/q1GjBnfu3KFWrVrcuXOH6tWrF2hjamrKkSNHVNuxsbE4\nOzurtr29vbG0tGTs2LHPPI+3tzfe3t5A3jR6UbSGDRzG7bjbZKRnMPzD4Qx+fzCWtSwZOnwoh4IP\nUb1mdabMnMLcmXOJi41j1IRRuPd253bMbWZ8PEO18OfkuZNpbtucVQtWcTT4KACJCYm06dCGWd/O\nop1FO45fP07kyUj8FvtRxbgK1/+8TmPrxsxZMQeFQsHxg8dZMmsJ+gb6NLdtTtytOJZuWFow6FyK\nfTKKTBQRpdnT//jS09Qr8unuxUUtQ4ienp6sX78eyJtp2LNnzwJt3N3dCQ4OJjExkcTERIKDg3F3\ndwfyZiQmJSXx7bffqiNc8QyLVy4m6GgQ+8L2sc53HQnxCaSmpNK2Q1sORxymQoUKLPhqAVsCt7Bm\n0xrWrVgHgJGJEd/5f8fm4M34+PqwcMZCAD6c9CFbQrfgt8OPSlUq0X9Y/wLn/PPin0yYPYGAsADi\nbsVxNuIsGekZzJs0j+WblrPpwCYexT9S699BCPFmUMs0+ilTptCvXz/Wrl1L3bp12bZtGwCRkZH4\n+vqyZs0ajI2NmTFjBra2tgDMnDkTY2NjYmNjmTt3Lo0aNaJVq1YAjBkzhhEjRqgjdPEv63zXsX9P\n3gzQ23G3iY6KRkdHh45ueZNsGjVphI6uDtra2jRu2pi7t/OGlLOzs1kwbQFXLl1BU0OTm3/dVPWZ\nm5vL9I+nM9h7MI2tGxc4Z7MWzahRuwYAVk2tuBN7BwNDA0zrmmJaxxQA93fc+XnTz8V67UKIN49a\nEpiJiQkHDx4ssN/GxoY1a9aotocNG8awYcPytTEzM6OMLhZSqpw8dpJjR46xO3Q3+gb69PHoQ0ZG\nBlraWqpnmhoaGqoFQDU0NFBmKwHY7LcZ46rG+If6k5OTg+Nbjqp+Vy9aTY1aNfAcUPjEDW1dbdVn\nTQ1NsrOznxtn2xZtOXH2xCtd2+5fdrNo7iKq1ahGwN6AZ7azb2bP/rD9GJsYv1L/QojiIStxiJfy\n5PETKlepjL6BPtevXufMr2de+tjkJ8lUr1UdDQ0N9mzbg1KZl9iOBh8l4lgEqwNWv1IsdRvUJe5m\nHLdjblPbvDbBu4Jf6fi/5ebmkpubi/8GfxYuX4idg91r9SOEKBllJoE9vfRRujKdq9FX1RpDWZ6N\n5uzqzMa1G3GycaKBZQNa2bZ66WP7evXlsw8+Y2/AXioZV0JbJ++u6utpX/Mo/hFDPYaSmpqKrqEu\nwz4dRnp6Ov269cOigYWqj7Yt2lLPtB5HjhxhR8AOPpr6EWPeHYOWlhYJSQlkZWSx8puV+c67Yc0G\ngnYEoZGtQZfuXZg4bSIxN2N493/v0tKmJRfOXqDH/3oQcTqCCaMn0NmjM1aNrTh/5jxzF+fNnh3a\ndyijPhmFY3tHhBBvljKTwJ5e+kjDQEPtSyGV5dlourq6/PTzTwX2X7tzTfV5wucT8n134NcDANSp\nX4etB7cCcP738/z0Q14/tS1rU61ONdb5r2Odb96Ej2WLlhFyPoRKlSrx0bCPGDB0AADpael4T/DG\nqZMTSxcsJSY2hp+P/cynIz/F+JEx7V3ao23wz3Dj6eOnuXXzFn4/+VHfvD7v9X+P0ydOY2pmSnRU\nNN/6fktru9ZA3vDojDkzaN6qOVs3bS2qP5kQopiVmQQmSocmzZrwx6U/SH6SjLaONo2aNOLShUuc\n+e0MHTp2oLVda4yM89bJ7NqjK2d+PUNH145oa2vToWMHABo1bcTm7zdzdM9Rrl+9jls3N3oN7oVS\nqWTporyp9KeOn+L08dMMCx+GIkdBaloq4ZHhtNZoTY06NahSowpRN/PeNUvLTiP2biwVblbgfsJ9\nklKTVN+lZKZw+/5tom5GkUUWN2JvkJicmO+aUhP/27tmZfnOXYjiJAlMqJWWtha1zWqz+5fdNG/Z\nHMuGlkRGRBJ7M5baZrX549IfhR6nqaWpmiyiqalJvYb1mD1/Nh3tOvLV8q/Q1NIk+ck/d8C55PL+\nyPfp4tYFAEMTQwBux97G0NhQtQ2gaaCJvpE+hiaGGBobommgqfpeqa1Er4oehiaGaOhrYGBsgKHR\nP8cCoPhv75qV5Tt3IYqTVGR+hpPHTjK079CSDqNMatW6FRvXbaSlbUta2rRkh/8OGjZpSLO3m3Hm\n1zM8SnyEUqkkaG+QapjvWZq3as6BvXlDlft3/7PIs2M7R3YF7FK9PH3/3n0S4hNeGFtts9pc+fMK\nOTk53L1zl0vnL/2HKxVCFCe5AxNq18KmBWt919K8RXP0DfTR0dGhpU1LqlavyscTPsZ7iDfkQjvn\ndji7OD+3r8+mfcbnEz7nx+9/xMnFSbW/Tbs2/BX1F6PeGwXk3YHNWTgHTU3N5/bXvFVzTM1M6ePR\nh7cavEXjJgXfTRNCvBnKTEXmwqqKfr/6ewb1GkQr21ZEhkfSolUL+g3ux+J5i3n44CEr1qwAYObk\nmWRkZKCnp8eSVUuwsLTg5LGT+C7zZcP2DaSmpDL9s+lcuXyFrOwsJkydgHs39wIxFEfF0ZLy9N/z\ndUTdisLQ2PDFDYtRSnwKQL4hQ4DbMbcZO3Qs2w5ve61+Vy1YRas2rbDvYE9KQgoN6jR47RjL0v9v\nROlXmqrZl/k7sBt/3WD1htUs+W4JHs4e7Ny+k53BOwneF8zyxctZunopvxz4BS0tLY4ePsr82fP5\n/qfv8/WxdNFS2nZoy5LvlpD0KIluHbvR3rk9BoYGJXRVxe/pBT5fR2pCKpTwP490tXXJyMoo0j6V\nSiUfTvqwSPsUQry6Mp/AzOua07hp3jCQVSMr2jm1Q6FQ0KhJI2JuxfD48WPGjhpLdFQ0CoWCrKys\nAn0cPXSUkH0h+C73BSAjI4O42DgsG1qq9VrUqahmxb0JVZ3/nlH4tOzsbKaNnsafF/6kQcMGzF46\nmwu/XeDbL78lW5lN0+ZNmeozFR1dHbrbdaezZ2dOHz2N10denDxykvau7XHt7kq/zv0YOGQgIUEh\nZGdls3rDaiysLIh/GM/o4aO5d+cere1ac/TwUYKOBslKHkIUkTI/iePvpY0gb3kjHV0d1WdltpKF\ncxbi2N6RQ+GH+HHrj2RkFPzXem5uLn4/+RFyIoSQEyH8evnXMp28youbUTfp69WXHUd3YFjBkE1+\nm5g1dhZf+37NtkPbUCqVBGz4Z2mpykaV2Ry8Gfd3Cg4fG5sYc+DYAYYMH4Lvsrx/6Cz5eolqoeNu\nPbsRFxNX4DghxOsr8wnsRZ48fkLN2jUB2Lap8OchTi5O/OD7g2pNxovnLqotPlF8atSuQQu7FgB0\n7d2ViOMR1K5Tm7oN6gLQvW93zpz+Z8mszj07P7Ovrp5dAbBuYU3MrRgAIk5H0LN3XuWFjm4dqVKl\nSrFchxDlVZkfQnyRDz/9kLGjxrJ04VJcOhde6XnspLF8MeULXB1cycnJwbyuORu2b1BzpOJpL1sw\nM+Z2DPrp+vn23b97H2WOkptxeSvj3394H00dTdKS0lT77j28R2paKjfjbpKtzOZh0kMyFZkApKSk\n8CDhATfjbqJUKlV3+pqamqpFjIUQxatMJzDzuuYcCj+k2v7W99tCvzv++3HV/skzJwPg2N5Rtf6d\nvr4+C5YuUEfIZUpRTAR5noR7CRgYveREmqcnk+TCw7sPuXLuCg2tG3I06CgWjSwI/iWYO7fuUMu8\nFmH7wmjaquk/x+Y+1c/T20+xtbdl9y+7GT1uNGEHw3j0SOqWCVGUynQCEyVLHcsjvewkkaen0Wvn\nalO3QV2O7jvKap/V1Leqz0cTP6J9x/Z8O/OfSRwffPwBOro6aGlqYVbLDCOTvGWuDA0NqWZSjbpm\ndZ/5btn4qeP5aNhH7PDfQWu71lSvUR3DCiX7WoEQZUmZfg8sKChIrTHI+zzq9bLvqkXdjCqQwIrS\ns94Dy8jIQFNTEy0tLSLDI5k6fiohJ0IKtJP/34g3ibwHJkQZcuWPKzy494B2zu1e6bi4mDhGvTeK\nnJwcdLR1WLhsYTFFKET5pJZZiAkJCbi5uWFpaYmbmxuJiYmFtlu/fj2WlpZYWlqyfv36At97enrS\nrFmz4g5XlGGvM8Hiyh9XOBH2alWeAepb1Cf4eDChJ0PZF7aPFq1bvHIfQohnU8sdmI+PDy4uLkyZ\nMgUfHx98fHyYP39+vjYJCQnMnj2byMhIFAoFrVu3xtPTEyOjvGcOP//8MxUqlPxLseLN9s38b/h5\n68+YVDWhtmltrFtas3vPbhq1bsS5M+dw7+ZO93e6M++Ledy9fReAidMm0rxVcy6eu8jCuQvJzMxE\nV1eXWT6zMDU1xXeZLxnpGfx+5nfe936/0GXEhBDqp5YEFhgYyJEjRwDw8vLC2dm5QAI7cOAAbm5u\nGBvnrVLg5uZGUFAQAwcOJDk5mSVLluDn50e/fv0KPcfTM95yUnPUXqZCT0dPrecT+Z397Sz7du0j\n5GTeihju7d2xbmkNQHZmNj/tyCuk+fn4zxn03iBatG7B3dt3GT18NDv276Be/Xqs27wOTS1NTp88\nzcolK1m4fCGjPhnFHxf+YPIXk0vy8oQQT1FLArt37x61atUCoGbNmty7d69Am7i4OMzNzVXbZmZm\nxMXlrVwwY8YMJkyYgIHBs6dMPz3jTU9TTx6MlzO/nv4Vdw939PT0QA/curqpvuvc7Z+XkCNORfBX\n1F+q7ZTkFFJTUklOTuaLKV8QcyMGFHlLTQkh3lxFlsBcXV25e/dugf1z587Nt61QKFSFCV/G2bNn\niYqK4ptvvuHGjRvPbevn54efnx8ADx48eOlziLJNI1cD0v9ZmV6ZouS7Vd/lW2YsNz2XZXOWYd3Y\nmi+//JI7t+/wyQefkBKfQkZSBlnJWarjn5aamEqywevf7cuduxCvp8gSWGho6DO/q1GjBnfu3KFW\nrVrcuXOH6tWrF2hjamqqGmYEiI2NxdnZmVOnThEZGUm9evXIzs7m/v37ODs752v7N29vb7y9vYG8\nqaCifLFtY8vksZMZM2EMymwloUGhDH5/MLraupjVNKNB3byp7p2cOxG2P4wPP81bUf7i+Ys0s26G\nIktBs8bNaFC3ATs37UQbbRrUbUD9OvW5fvG66vinJRvKNHghSoJaZiF6enqqZhWuX7+enj17Fmjj\n7u5OcHAwiYmJJCYmEhwcjLu7Ox9++CG3b9/mxo0bHD9+HCsrq0KTlxAtWregc9fOuDq4Mrj3YBo3\nbUzFShULtPtq4Vec+/0crg6uONs6s3HdRiBvWbGvZ31N53ad8w0fOrZ35Nqf13Br60bgjkC1XY8Q\n4vnU8iJzfHw8/fr149atW9StW5dt27ZhbGxMZGQkvr6+rFmzBoB169Yxb948AKZNm8b777+fr58b\nN27QvXt3Ll588WK6pellPPF6CnuROSU5BcMKhqSlptGray8WLF3A2y3eLtY45EVkUZaUpt/OMrMS\nx9NK0/8I4vUUlsBGDxvN1StXyUjPoO+7ffl4wsfFHockMFGWlKbfTlmJQ5QpK9etLOkQhBBqIglM\niCJ0K+4W6ZkvLvFSUvR09NSyyLIQ6iAJTIgilJ6Z/tIr5JcEdb/cL0RxKvcVmYUQQpROcgcmSq3i\nLpj5KnEIIdRPEpgoteRZjhDlmwwhCiGEKJUkgQmhBksXLqVdy3a80/kdPnr/I3yX+dLHow/nzpwD\nICE+Aftm9gAolUq+mv4VHk4euDq4qlYKAVi1dJVq/6K5iwCIuRmDk40Tn338GR3tOjKw50DS0tLU\nf5FCqJkkMCGK2fnfz7Nrxy5CToSwMWCjKmk9y5YNW6hYqSL7wvax98heNq/fzK0btwg7GEZ0VDR7\nj+wl+EQw58+e5/SJ0wBER0Xj9YEXhyMOU6lKJfYF7lPHpQlRouQZmBDFLPxkOF26d0HfQB8ANw+3\n57YPOxTGHxf/YG/gXgCePH5CdFQ0YYfCCDsURud2eaVhUpNTiY6KxtTMFPO65jSzzqtWbt3Cmphb\nMcV4RUK8GSSBCVFCNLU0ycnJASA9/V8vP+fCnIVzcHZ1ztf+yMEjjBk/hiHDhuTbH3MzJl9pGE1N\nTdLT3tyXqYUoKjKEKEQxa9O2DQf2HiAtLY3kJ8mE7A8BwLyOOefPngdg7869qvZOLk5sWLuBrKws\nAKKuRZGakoqzizNbN24lJTmvLtmd23d4+OChmq9GiDeH3IEJUczebvE2PXr1wM3RjarVqtKiVQsA\nRn0yilFeo9j04yZcOruo2r/r9S4xt2Lo0r4Lubm5GFc1Zt3mdTi5OHHtyjU8XT0BMDA0YPn3y9HU\n1CyR6xKipMlq9EIUocJWyH/a4nmLMaxgyKhPRqkpqn/IyvniRUrTb6fcgQnxL/91Md7o2GgMUg2e\n2yYhKYG0rDSibkW9VJ962nqY1jJ97ZiEKKskgQnxL/91MV6DFAMMjQ2f2+bjGa9WoywlIeW14xGi\nLJNJHEIIIUolSWBCCCFKJbUksISEBNzc3LC0tMTNzY3ExMRC261fvx5LS0ssLS1Zv369an9mZibe\n3t5YWVnRqFEjduzYoY6whShg8bzF+C7z/c/9eA/25vKFy0UQkRDll1qegfn4+ODi4sKUKVPw8fHB\nx8eH+fPn52uTkJDA7NmziYyMRKFQ0Lp1azw9PTEyMmLu3LlUr16dq1evkpOTQ0JCgjrCFuKV6Wrr\nkhL/4mdWylQlaYlpL9U2NTGVZIOiKRsjpV9EWaKWBBYYGMiRI0cA8PLywtnZuUACO3DgAG5ubhgb\nGwPg5uZGUFAQAwcOZN26dfz5558AaGhoULVqVXWELQSQtxDv9s3bqVqtKrVNa2Pd0po+Hn2YMWcG\nzVs1JyE+ga5OXQm/3dJw9wAADdNJREFUGM6Jwyc4sOcAqal5yzyN+ngUmVmZ7PDfgY6ODhsDNmJk\nbIS+lj7hR8L5du63KLOVLF65mJY2LQs9f7KhTH0XojBqGUK8d+8etWrVAqBmzZrcu3evQJu4uDjM\nzc1V22ZmZsTFxfHo0SMAZsyYQatWrejbt2+hxwtRHF51IV6AK5evsOanNew7so/5X81HX1+f4OPB\ntLZrTcCWAFW7tLQ0Qk6EMG/JPCaMnlCclyFEmVRkCczV1ZVmzZoV+C8wMDBfO4VCgUKheOl+s7Oz\niY2NxdHRkTNnzuDg4MDEiRMLbevn54eNjQ02NjY8ePDgP12PEJB/Id6KlSq+cCFeAMcOjlSoWAGT\nqiZ5x3TNO6Zx08b5Ftnt2acnkLfU1JMnT0h6lFQ8FyFEGVVkQ4ihoaHP/K5GjRr8X3v3H9PknccB\n/F0ZhVXFWBDFwuEoNYFBOyc/3PxxjILVLUK2U2KWOXKy4RyZ23KR+A/+SHRobobME3FO5rrt1KHZ\nLDkdQ9xGzuiJZIpTcydjoNJ1nJQOJ46ffu8Pk54KhOLgefrQ9yshecrzbfvuJ4VP2+fb7+NwOBAW\nFgaHw4HQ0NB+Y3Q6nftjRgBobm5GSkoKgoODodFo8MILLwAAli1bhtLS0gHvJzc3F7m5uQDufpuc\naLQMuhAvALVa7d4eN26ce6Fd1TgV+nr73PsefCE3nBd2RCTRR4gZGRnuWYVWqxWZmZn9xlgsFlRW\nVsLlcsHlcqGyshIWiwUqlQpLlixxN7cTJ04gNjZWithEw16IdzjKPy8HANScrkFQUBCCJgWNTGgi\nHyHJJI5169YhKysLpaWliIyMRFlZGQCgtrYWu3fvxt69e6HValFQUIDExEQAwPr1690TOrZt24YV\nK1bgrbfewpQpU7Bv3z4pYhMNeyHe4QgICMDCeQvR29OL7cXbRzI2kU/gYr5E9/BkMV6pcQFekpKS\n/ndyJQ4iIlIkNjAiIlIkNjAiIlIkNjAiIlIkNjAiIlIkntCS6B6B6kDcah2ZhXNHChfgJRoYGxjR\nPf6g+4PcEYjIQ/wIkYiIFIkNjIiIFIkNjIiIFIkNjIiIFIkNjIiIFIkNjIiIFIkNjIiIFIkNjIiI\nFIkNjIiIFIkNjIiIFEmSBtbW1ob09HQYDAakp6fD5XINOM5qtcJgMMBgMMBqtbp/f+DAAcTHx8No\nNGLRokVobW2VIjYREXkxSRrY1q1bYTabUV9fD7PZjK1bt/Yb09bWhk2bNuHMmTOoqanBpk2b4HK5\n0NvbizfffBPffPMNLly4AKPRiJ07d0oRm4iIvJgkDcxmsyE7OxsAkJ2djSNHjvQb89VXXyE9PR1a\nrRaTJ09Geno6KioqIISAEAIdHR0QQuDmzZuYPn26FLGJiMiLSbIafUtLC8LCwgAA06ZNQ0tLS78x\ndrsdERER7svh4eGw2+3w9/dHSUkJ4uPjMX78eBgMBhQXF0sRm4iIvNiIvQNLS0tDXFxcvx+bzXbf\nOJVKBZVK5fHt9vT0oKSkBOfOncNPP/0Eo9GIwsLCAcfu2bMHCQkJSEhIwI0bN37X4yEiIu82Yu/A\nqqqqBt03depUOBwOhIWFweFwIDQ0tN8YnU6Hb7/91n25ubkZKSkpOH/+PABAr9cDALKysgY8hgYA\nubm5yM3NBQAkJCQ87EMhIiIFkOQYWEZGhntWodVqRWZmZr8xFosFlZWVcLlccLlcqKyshMVigU6n\nw+XLl93vqI4fP46YmBgpYhMRkReT5BjYunXrkJWVhdLSUkRGRqKsrAwAUFtbi927d2Pv3r3QarUo\nKChAYmIiAGD9+vXQarUAgA0bNmDBggXw9/dHZGQkPvroIyli0xhwzX4Nnd2dkt1foDqQZ3UmkohK\nCCHkDjEaEhISUFtbK3cMktmVxiuYEDJBsvu71XoLMx+bKdn9EY00Jf3v5EocRESkSGxgRESkSGxg\nRB5a+uxS1H1XBwBIjktGm7NN5kREvo0NjIiIFEmSWYh0P6lnxgGcHXevkvdKoFarkbM6BxvWbcDl\ni5dx6B+HcLL6JA5+fBDLXlyGd995F93d3Yh8LBJFu4owfsJ4uWMT0QPYwGTQ2d0p6cw44O7sOLor\n6akkvL/zfeSszsGFcxfQ3dWNnp4e1JyqQUxcDN7763v4rPwzaMZrUFxUjD079+DtdW/LHZuIHsAG\nRj7HOMuI789/j19v/gp1gBrxpnjUfVeHM6fPYOHihbjy7yvIXHj3y/Y93T2YnTRb5sRENBA2MPI5\n/v7+iIiMQNnfy5CQlICYuBic+ucpNP3YhIgZEVjwzALs2rdL7phENARO4vAS169eR2py6n2/q/uu\nDgVrC2RKNLYlP5WM3X/bjeS5yUh+OhmffPgJ4oxxmJ04G2fPnEVjQyMA4HbHbTTUN8iclogGwndg\nXsz0pAmmJ01yxxiTkp5Owo53dyAhKQGa8RoEBAQg6ekkBIcEo6ikCHkr89Dd3Q0AyC/Ih96glzkx\nET2IDcwLXW28ildXvIrnlz2P0ydP4+NDH2P7O9thb7bjWtM12JvteGX1K8hZnQMAKNpWhM8/+xzB\nIcGYrpsO4ywjXlvzmsyPwrvNT5mPq21X3ZdPnjvp3p73x3k4Vn2s33UOHzvs3j5z8czoBiSiIbGB\neZkf6n/A639+HUUlRWj/pR2nT57+/74rP+DQ0UPouNWB+U/Ox8uvvIxLFy7hWPkxHD91HL09vbDM\nt8A4yyjjIyAikgaPgXkRZ6sTK5evxM69O/F4/OP99pstZgQEBEAbrEXIlBDc+O8NnP3XWVietSAw\nMBATJk5A+uJ0GZITEUmPDcyLTAyaCF24DjWnawbcHxAQ4N728/NDX2+fVNGIiLwOG5gXUavVKN1f\nisMHDuOLsi88uk7inEQcrziOzs5OdNzqQFXF4GfGJiIaS9jAvIxmvAbWMis+KP4At34devWMJ2Y/\ngYWLFyLtqTS89KeXEPN4DCYGTZQgKRGRvMbsCS1DQkIwY8YMuWMMqLOvE+M0/V87OJ1OBAcHD/v2\n+vr64Ofnhzt9d/DjxR+h0+vw6IRH7xtz5/YdBPoFPnRmud24cQNTpkwZ9vW6+rogIN1TXAUVAvwC\nhh44gh62Nr6AtRncYLVpampCa2urDImGb8w2MG822FmCFy1ahIqKimHfXt7KPFz5zxV0dXZh2YvL\n8MZf3ug3RulnClbSWWKlxtoMjrUZ3FioDafRjwHFHxbLHYGISHI8BkZERIrkt3Hjxo1yh/A1zl+c\nUGvUA+4zGkfnS8jdt7sRPHn4x9e8yezZXBV+MKzN4FibwSm9NjwGJoPBjoGNJqUfAyMiehCPgckg\nUB0o+QkmA9XKnYFIRDQgQbJxOp0iLS1NREdHi7S0NNHW1jbgOIvFIiZNmiSee+45iRNK78svvxQz\nZ84Uer1eFBYW9tvf2dkpsrKyhF6vF0lJSaKxsVH6kDIZqjbV1dVi1qxZws/PTxw6dEiGhPIZqjbb\nt28XMTExIj4+XqSmpoqmpiYZUspjqNqUlJSIuLg4YTKZxNy5c8WlS5dkSPlw2MBktHbtWvcTqrCw\nUOTn5w84rqqqSpSXl4/5Btbb2yuioqJEQ0OD6OrqEkajsd8fU3FxsVi1apUQQogDBw6IrKwsOaJK\nzpPaNDY2irq6OrFixQqfamCe1Obrr78WHR0dQgghdu3axefNPdrb293bNptNWCwWqWM+NM5ClJHN\nZkN2djYAIDs7G0eOHBlwnNlsxsSJY391jZqaGkRHRyMqKgpqtRrLly+HzWa7b8y9NVu6dClOnDgB\n4QOHcT2pzYwZM2A0GjFunG/9WXtSm2eeeQYajQYAMGfOHDQ3N8sRVXKe1CYoKMi93dHRAZVKJXXM\nh+Zbz3Qv09LSgrCwMADAtGnT0NLSInMiedntdkRERLgvh4eHw263DzrmkUcewaRJk+B0OiXNKQdP\nauOrhlub0tJSLF68WIposvO0NsXFxdDr9cjPz8eOHTukjPi7cBLHKEtLS8PPP//c7/dbtmy577JK\npVLUKx8iJfr0009RW1uL6upquaN4lby8POTl5WH//v3YvHkzrFar3JE8wgY2yqqqBl8dfurUqXA4\nHAgLC4PD4UBoaKiEybyPTqfD9evX3Zebm5uh0+kGHBMeHo7e3l60t7c/1PqRSuNJbXyVp7WpqqrC\nli1bUF1dfd+picay4T5vli9fjtWrV0sRbUTwI0QZZWRkuF/pWK1WZGZmypxIXomJiaivr0djYyO6\nu7tx8OBBZGRk3Dfm3podPnwYqampPvHO1ZPa+CpPanPu3DmsWrUK5eXlPvVC0ZPa1NfXu7ePHj0K\ng8EgdcyHJ/csEl/W2toqUlNTRXR0tDCbzcLpdAohhDh79qzIyclxj5s3b54ICQkRgYGBQqfTiYqK\nCrkij7qjR48Kg8EgoqKixObNm4UQQhQUFAibzSaEEOK3334TS5cuFXq9XiQmJoqGhgY540pqqNrU\n1NQInU4nNBqN0Gq1IjY2Vs64khqqNmazWYSGhgqTySRMJpNYsmSJnHElNVRt1qxZI2JjY4XJZBIp\nKSni4sWLcsYdFq7EQUREisSPEImISJHYwIiISJHYwIiISJHYwIiISJHYwIiISJHYwIiISJHYwIiI\nSJHYwIiISJHYwIiISJHYwIiISJHYwIiISJHYwIiISJHYwIiISJHYwIiISJHYwIiISJHYwIiISJHY\nwIiISJHYwIiISJHYwIiISJHYwIiISJH+B+yCDnv5Stz4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26S1DmkzA2-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}